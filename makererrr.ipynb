{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport cv2\nimport ast\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\n\nimport random\n\nimport torch\n\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-09T20:05:18.688651Z","iopub.execute_input":"2022-09-09T20:05:18.689323Z","iopub.status.idle":"2022-09-09T20:05:18.695582Z","shell.execute_reply.started":"2022-09-09T20:05:18.689289Z","shell.execute_reply":"2022-09-09T20:05:18.694642Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!pip install albumentations==0.4.6\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:05:18.700799Z","iopub.execute_input":"2022-09-09T20:05:18.701796Z","iopub.status.idle":"2022-09-09T20:05:27.741170Z","shell.execute_reply.started":"2022-09-09T20:05:18.701713Z","shell.execute_reply":"2022-09-09T20:05:27.739985Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations==0.4.6 in /opt/conda/lib/python3.7/site-packages (0.4.6)\nRequirement already satisfied: opencv-python>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (4.5.4.60)\nRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (0.4.0)\nRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (1.21.6)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (6.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (1.7.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (9.1.1)\nRequirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.19.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.5.3)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.19.3)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (21.3)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.33.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.3.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (5.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/makerere-passion-fruit-disease-detection-challenge/Train (11).csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:05:27.745382Z","iopub.execute_input":"2022-09-09T20:05:27.745711Z","iopub.status.idle":"2022-09-09T20:05:27.769742Z","shell.execute_reply.started":"2022-09-09T20:05:27.745668Z","shell.execute_reply":"2022-09-09T20:05:27.768663Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"      Image_ID            class   xmin   ymin  width  height\n0  ID_007FAIEI  fruit_woodiness   87.0   87.5  228.0   311.0\n1  ID_00G8K1V3  fruit_brownspot   97.5   17.5  245.0   354.5\n2  ID_00WROUT9  fruit_brownspot  156.5  209.5  248.0   302.5\n3  ID_00ZJEEK3    fruit_healthy  125.0  193.0  254.5   217.0\n4  ID_018UIENR  fruit_brownspot   79.5  232.5  233.5   182.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>width</th>\n      <th>height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_007FAIEI</td>\n      <td>fruit_woodiness</td>\n      <td>87.0</td>\n      <td>87.5</td>\n      <td>228.0</td>\n      <td>311.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_00G8K1V3</td>\n      <td>fruit_brownspot</td>\n      <td>97.5</td>\n      <td>17.5</td>\n      <td>245.0</td>\n      <td>354.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_00WROUT9</td>\n      <td>fruit_brownspot</td>\n      <td>156.5</td>\n      <td>209.5</td>\n      <td>248.0</td>\n      <td>302.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_00ZJEEK3</td>\n      <td>fruit_healthy</td>\n      <td>125.0</td>\n      <td>193.0</td>\n      <td>254.5</td>\n      <td>217.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_018UIENR</td>\n      <td>fruit_brownspot</td>\n      <td>79.5</td>\n      <td>232.5</td>\n      <td>233.5</td>\n      <td>182.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"No_duplicates = train_df.drop_duplicates(subset=\"Image_ID\")\nprint(No_duplicates.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:05:27.772848Z","iopub.execute_input":"2022-09-09T20:05:27.773119Z","iopub.status.idle":"2022-09-09T20:05:27.784347Z","shell.execute_reply.started":"2022-09-09T20:05:27.773095Z","shell.execute_reply":"2022-09-09T20:05:27.783058Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(3001, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/makerere-passion-fruit-disease-detection-challenge/Test (12).csv\")\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:05:27.787458Z","iopub.execute_input":"2022-09-09T20:05:27.787800Z","iopub.status.idle":"2022-09-09T20:05:27.807694Z","shell.execute_reply.started":"2022-09-09T20:05:27.787770Z","shell.execute_reply":"2022-09-09T20:05:27.806736Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"      Image_ID\n0  ID_IUJJG62B\n1  ID_ZPNDRD4T\n2  ID_AHFYB64P\n3  ID_L8JZLNTF\n4  ID_IFMUXGPL","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_IUJJG62B</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_ZPNDRD4T</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_AHFYB64P</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_L8JZLNTF</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_IFMUXGPL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df['xmax'] = train_df['xmin']+train_df['width']\ntrain_df['ymax'] = train_df['ymin']+train_df['height']","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:05:27.809008Z","iopub.execute_input":"2022-09-09T20:05:27.809408Z","iopub.status.idle":"2022-09-09T20:05:27.818257Z","shell.execute_reply.started":"2022-09-09T20:05:27.809375Z","shell.execute_reply":"2022-09-09T20:05:27.817252Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"classes_la = {\"fruit_brownspot\": 1, \"fruit_healthy\": 2, \"fruit_woodiness\":3}\n\ntrain_df[\"class\"] = train_df[\"class\"].apply(lambda x: classes_la[x])","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:05:27.821136Z","iopub.execute_input":"2022-09-09T20:05:27.821550Z","iopub.status.idle":"2022-09-09T20:05:27.829059Z","shell.execute_reply.started":"2022-09-09T20:05:27.821517Z","shell.execute_reply":"2022-09-09T20:05:27.828074Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df = train_df.copy() # create a copy of the train df\n","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:05:27.830542Z","iopub.execute_input":"2022-09-09T20:05:27.830916Z","iopub.status.idle":"2022-09-09T20:05:27.840979Z","shell.execute_reply.started":"2022-09-09T20:05:27.830883Z","shell.execute_reply":"2022-09-09T20:05:27.840039Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:05:27.843733Z","iopub.execute_input":"2022-09-09T20:05:27.844908Z","iopub.status.idle":"2022-09-09T20:05:27.860633Z","shell.execute_reply.started":"2022-09-09T20:05:27.844873Z","shell.execute_reply":"2022-09-09T20:05:27.859615Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"      Image_ID  class   xmin   ymin  width  height   xmax   ymax\n0  ID_007FAIEI      3   87.0   87.5  228.0   311.0  315.0  398.5\n1  ID_00G8K1V3      1   97.5   17.5  245.0   354.5  342.5  372.0\n2  ID_00WROUT9      1  156.5  209.5  248.0   302.5  404.5  512.0\n3  ID_00ZJEEK3      2  125.0  193.0  254.5   217.0  379.5  410.0\n4  ID_018UIENR      1   79.5  232.5  233.5   182.0  313.0  414.5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>width</th>\n      <th>height</th>\n      <th>xmax</th>\n      <th>ymax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_007FAIEI</td>\n      <td>3</td>\n      <td>87.0</td>\n      <td>87.5</td>\n      <td>228.0</td>\n      <td>311.0</td>\n      <td>315.0</td>\n      <td>398.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_00G8K1V3</td>\n      <td>1</td>\n      <td>97.5</td>\n      <td>17.5</td>\n      <td>245.0</td>\n      <td>354.5</td>\n      <td>342.5</td>\n      <td>372.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_00WROUT9</td>\n      <td>1</td>\n      <td>156.5</td>\n      <td>209.5</td>\n      <td>248.0</td>\n      <td>302.5</td>\n      <td>404.5</td>\n      <td>512.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_00ZJEEK3</td>\n      <td>2</td>\n      <td>125.0</td>\n      <td>193.0</td>\n      <td>254.5</td>\n      <td>217.0</td>\n      <td>379.5</td>\n      <td>410.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_018UIENR</td>\n      <td>1</td>\n      <td>79.5</td>\n      <td>232.5</td>\n      <td>233.5</td>\n      <td>182.0</td>\n      <td>313.0</td>\n      <td>414.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_grp = df.groupby(['Image_ID'])","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:05:27.862210Z","iopub.execute_input":"2022-09-09T20:05:27.862797Z","iopub.status.idle":"2022-09-09T20:05:27.867572Z","shell.execute_reply.started":"2022-09-09T20:05:27.862762Z","shell.execute_reply":"2022-09-09T20:05:27.866587Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class PassionFruit(object):\n    def __init__(self, df, IMG_DIR, transforms): \n        self.df = df\n        self.img_dir = IMG_DIR\n        self.image_ids = self.df['Image_ID'].unique().tolist()\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_values = self.df[self.df['Image_ID'] == image_id]\n        image = cv2.imread(str(self.img_dir)+str(image_id)+\".jpg\",cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        \n        boxes = image_values[['xmin', 'ymin', 'xmax', 'ymax']].to_numpy()\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        \n        labels = image_values[\"class\"].values\n        labels = torch.tensor(labels)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([idx])\n        target['area'] = torch.as_tensor(area, dtype=torch.float32)\n        target['iscrowd'] = torch.zeros(len(classes_la), dtype=torch.int64)\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n        \n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return torch.tensor(image), target, image_id","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:14:26.222153Z","iopub.execute_input":"2022-09-09T20:14:26.222535Z","iopub.status.idle":"2022-09-09T20:14:26.236505Z","shell.execute_reply.started":"2022-09-09T20:14:26.222491Z","shell.execute_reply":"2022-09-09T20:14:26.235167Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"pip install -U albumentations","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:14:26.914511Z","iopub.execute_input":"2022-09-09T20:14:26.915314Z","iopub.status.idle":"2022-09-09T20:14:29.465883Z","shell.execute_reply.started":"2022-09-09T20:14:26.915275Z","shell.execute_reply":"2022-09-09T20:14:29.464370Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        # A.HorizontalFlip(p=0.5),\n        # A.VerticalFlip(p=0.5),\n        A.RandomBrightness(),\n        A.RandomRotate90(),\n        A.Rotate(limit=(-90, 90)),\n        A.Transpose(),\n        A.Downscale (),\n        A.RandomContrast(),\n        A.RandomBrightnessContrast(),\n        A.RandomGamma(),\n        A.Blur(),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:14:31.419557Z","iopub.execute_input":"2022-09-09T20:14:31.419955Z","iopub.status.idle":"2022-09-09T20:14:31.427726Z","shell.execute_reply.started":"2022-09-09T20:14:31.419920Z","shell.execute_reply":"2022-09-09T20:14:31.426586Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import path\npassion_dataset = PassionFruit(df, path, get_train_transform())","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:14:32.024990Z","iopub.execute_input":"2022-09-09T20:14:32.025568Z","iopub.status.idle":"2022-09-09T20:14:32.032808Z","shell.execute_reply.started":"2022-09-09T20:14:32.025538Z","shell.execute_reply":"2022-09-09T20:14:32.031732Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"image_ids = df['Image_ID'].unique()\nvalid_ids = image_ids[-665:]\ntrain_ids = image_ids[:-665]\nvalid_df = df[df['Image_ID'].isin(valid_ids)]\ntrain_df = df[df['Image_ID'].isin(train_ids)]\ntrain_df.shape,valid_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:14:33.262934Z","iopub.execute_input":"2022-09-09T20:14:33.265562Z","iopub.status.idle":"2022-09-09T20:14:33.278849Z","shell.execute_reply.started":"2022-09-09T20:14:33.265525Z","shell.execute_reply":"2022-09-09T20:14:33.277739Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"((3054, 8), (852, 8))"},"metadata":{}}]},{"cell_type":"code","source":"path=\"../input/makerere-passion-fruit-disease-detection-challenge/Train_Images/Train_Images/\"","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:19:14.195996Z","iopub.execute_input":"2022-09-09T20:19:14.196680Z","iopub.status.idle":"2022-09-09T20:19:14.202117Z","shell.execute_reply.started":"2022-09-09T20:19:14.196637Z","shell.execute_reply":"2022-09-09T20:19:14.201014Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = PassionFruit(df, path, get_train_transform())\nvalid_dataset = PassionFruit(df, path, get_valid_transform())\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:19:14.983180Z","iopub.execute_input":"2022-09-09T20:19:14.984151Z","iopub.status.idle":"2022-09-09T20:19:14.995628Z","shell.execute_reply.started":"2022-09-09T20:19:14.984114Z","shell.execute_reply":"2022-09-09T20:19:14.994590Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"num_classes = 4 # + background\n\n# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:19:15.515799Z","iopub.execute_input":"2022-09-09T20:19:15.516149Z","iopub.status.idle":"2022-09-09T20:19:16.388529Z","shell.execute_reply.started":"2022-09-09T20:19:15.516120Z","shell.execute_reply":"2022-09-09T20:19:16.387553Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:19:16.390964Z","iopub.execute_input":"2022-09-09T20:19:16.391566Z","iopub.status.idle":"2022-09-09T20:19:16.397373Z","shell.execute_reply.started":"2022-09-09T20:19:16.391519Z","shell.execute_reply":"2022-09-09T20:19:16.396192Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.009, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:19:17.218250Z","iopub.execute_input":"2022-09-09T20:19:17.219424Z","iopub.status.idle":"2022-09-09T20:19:17.278218Z","shell.execute_reply.started":"2022-09-09T20:19:17.219379Z","shell.execute_reply":"2022-09-09T20:19:17.277214Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"num_epochs = 5","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:19:18.140877Z","iopub.execute_input":"2022-09-09T20:19:18.141843Z","iopub.status.idle":"2022-09-09T20:19:18.146837Z","shell.execute_reply.started":"2022-09-09T20:19:18.141801Z","shell.execute_reply":"2022-09-09T20:19:18.145568Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"import sys\nbest_epoch = 0\nmin_loss = sys.maxsize\n\nfor epoch in range(num_epochs):\n    tk = tqdm(train_data_loader)\n    model.train();\n    for images, targets, image_ids in tk:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        tk.set_postfix(train_loss=loss_value)\n    tk.close()\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    \n    print(f\"Epoch #{epoch} loss: {loss_value}\") \n        \n    #validation \n    model.eval();\n    with torch.no_grad():\n        tk = tqdm(valid_data_loader)\n        for images, targets, image_ids in tk:\n        \n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            val_output = model(images)\n            val_output = [{k: v.to(device) for k, v in t.items()} for t in val_output]\n            IOU = []\n            for j in range(len(val_output)):\n                a,b = val_output[j]['boxes'].cpu().detach(), targets[j]['boxes'].cpu().detach()\n                chk = torchvision.ops.box_iou(a,b)\n                res = np.nanmean(chk.sum(axis=1)/(chk>0).sum(axis=1))\n                IOU.append(res)\n            tk.set_postfix(IoU=np.mean(IOU))\n        tk.close()","metadata":{"execution":{"iopub.status.busy":"2022-09-09T20:19:18.758944Z","iopub.execute_input":"2022-09-09T20:19:18.759530Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/376 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a8f9c4afa5d4b799de66c884b7500e8"}},"metadata":{}}]},{"cell_type":"code","source":"img,target,_ = valid_dataset[5]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['boxes']))\nprint('real #boxes: ', len(target['boxes']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/makerere-passion-fruit-disease-detection-challenge/Test (12).csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(object):\n    def __init__(self, df, IMG_DIR, transforms):        \n        self.df = df\n        self.img_dir = IMG_DIR\n        self.transforms = transforms\n        self.image_ids = self.df['Image_ID'].tolist()\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):        \n        image_id = self.image_ids[idx]\n        image = cv2.imread(self.img_dir+image_id+\".jpg\",cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transform(IMG_SIZE=(512,512)):\n    return A.Compose([\n         A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n        A.Resize(*IMG_SIZE),\n        ToTensorV2(p=1.0)\n    ])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img_dir = \"../input/makerere-passion-fruit-disease-detection-challenge/Test_Images (1)/Test_Images/\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = (512,512)\ntest_dataset = TestDataset(submission, test_img_dir ,get_test_transform())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\nfor j in range(submission.shape[0]):\n    \n    img,_ = test_dataset[j]\n    img = img.unsqueeze_(0)\n    # put the model in evaluation mode\n    model.eval()\n\n    with torch.no_grad():\n        prediction = model([img.to(device)][0])\n        aa = zip(prediction[0][\"boxes\"].tolist(), prediction[0][\"labels\"].tolist(), prediction[0][\"scores\"].tolist())\n       \n        for item in list(aa):\n            row_dict = {}\n            row_dict[\"Image_ID\"] = _\n            row_dict[\"boxes\"] = item[0]\n            row_dict[\"labels\"] = item[1]\n            row_dict[\"confidence\"] = item[2]\n            results.append(row_dict)\nsub_df = pd.DataFrame(results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df[\"ymin\"] = sub_df[\"boxes\"].apply(lambda x: x[1])\nsub_df[\"xmin\"] = sub_df[\"boxes\"].apply(lambda x: x[0])\nsub_df[\"ymax\"] = sub_df[\"boxes\"].apply(lambda x: x[3])\nsub_df[\"xmax\"]=  sub_df[\"boxes\"].apply(lambda x: x[2])\nclasses_la = {0:\"Background\", 1:\"fruit_brownspot\", 2:\"fruit_healthy\", 3:\"fruit_woodiness\"}\nsub_df[\"labels\"] = sub_df[\"labels\"].apply(lambda x: classes_la[x])\nsub_df.drop([\"boxes\"], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.rename(columns={\"labels\":\"class\"}, inplace=True)\nsub_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"Submission_20fgsdfg.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}