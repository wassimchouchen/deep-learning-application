{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport cv2\nimport ast\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\n\nimport random\n\nimport torch\n\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-10T10:22:14.297823Z","iopub.execute_input":"2022-09-10T10:22:14.298651Z","iopub.status.idle":"2022-09-10T10:22:16.707357Z","shell.execute_reply.started":"2022-09-10T10:22:14.298560Z","shell.execute_reply":"2022-09-10T10:22:16.706259Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install albumentations==0.4.6\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:16.709799Z","iopub.execute_input":"2022-09-10T10:22:16.710847Z","iopub.status.idle":"2022-09-10T10:22:32.361892Z","shell.execute_reply.started":"2022-09-10T10:22:16.710806Z","shell.execute_reply":"2022-09-10T10:22:32.360760Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting albumentations==0.4.6\n  Downloading albumentations-0.4.6.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m852.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (1.7.3)\nRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (6.0)\nRequirement already satisfied: opencv-python>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (4.5.4.60)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.19.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.5.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\nRequirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.19.3)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (9.1.1)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (21.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.33.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.3.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (5.1.1)\nBuilding wheels for collected packages: albumentations\n  Building wheel for albumentations (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=c84fa1c934873dea825fbbea4cb1e8d884e3e85e5ae1b8d8e1b4278e7f499fc8\n  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\nSuccessfully built albumentations\nInstalling collected packages: albumentations\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.2.1\n    Uninstalling albumentations-1.2.1:\n      Successfully uninstalled albumentations-1.2.1\nSuccessfully installed albumentations-0.4.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/makerere-passion-fruit-disease-detection-challenge/Train (11).csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.363703Z","iopub.execute_input":"2022-09-10T10:22:32.364099Z","iopub.status.idle":"2022-09-10T10:22:32.401401Z","shell.execute_reply.started":"2022-09-10T10:22:32.364062Z","shell.execute_reply":"2022-09-10T10:22:32.400421Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      Image_ID            class   xmin   ymin  width  height\n0  ID_007FAIEI  fruit_woodiness   87.0   87.5  228.0   311.0\n1  ID_00G8K1V3  fruit_brownspot   97.5   17.5  245.0   354.5\n2  ID_00WROUT9  fruit_brownspot  156.5  209.5  248.0   302.5\n3  ID_00ZJEEK3    fruit_healthy  125.0  193.0  254.5   217.0\n4  ID_018UIENR  fruit_brownspot   79.5  232.5  233.5   182.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>width</th>\n      <th>height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_007FAIEI</td>\n      <td>fruit_woodiness</td>\n      <td>87.0</td>\n      <td>87.5</td>\n      <td>228.0</td>\n      <td>311.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_00G8K1V3</td>\n      <td>fruit_brownspot</td>\n      <td>97.5</td>\n      <td>17.5</td>\n      <td>245.0</td>\n      <td>354.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_00WROUT9</td>\n      <td>fruit_brownspot</td>\n      <td>156.5</td>\n      <td>209.5</td>\n      <td>248.0</td>\n      <td>302.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_00ZJEEK3</td>\n      <td>fruit_healthy</td>\n      <td>125.0</td>\n      <td>193.0</td>\n      <td>254.5</td>\n      <td>217.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_018UIENR</td>\n      <td>fruit_brownspot</td>\n      <td>79.5</td>\n      <td>232.5</td>\n      <td>233.5</td>\n      <td>182.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"No_duplicates = train_df.drop_duplicates(subset=\"Image_ID\")\nprint(No_duplicates.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.404230Z","iopub.execute_input":"2022-09-10T10:22:32.404645Z","iopub.status.idle":"2022-09-10T10:22:32.422177Z","shell.execute_reply.started":"2022-09-10T10:22:32.404604Z","shell.execute_reply":"2022-09-10T10:22:32.421244Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(3001, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/makerere-passion-fruit-disease-detection-challenge/Test (12).csv\")\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.423601Z","iopub.execute_input":"2022-09-10T10:22:32.424059Z","iopub.status.idle":"2022-09-10T10:22:32.441808Z","shell.execute_reply.started":"2022-09-10T10:22:32.424023Z","shell.execute_reply":"2022-09-10T10:22:32.440812Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      Image_ID\n0  ID_IUJJG62B\n1  ID_ZPNDRD4T\n2  ID_AHFYB64P\n3  ID_L8JZLNTF\n4  ID_IFMUXGPL","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_IUJJG62B</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_ZPNDRD4T</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_AHFYB64P</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_L8JZLNTF</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_IFMUXGPL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df['xmax'] = train_df['xmin']+train_df['width']\ntrain_df['ymax'] = train_df['ymin']+train_df['height']","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.443291Z","iopub.execute_input":"2022-09-10T10:22:32.443915Z","iopub.status.idle":"2022-09-10T10:22:32.450378Z","shell.execute_reply.started":"2022-09-10T10:22:32.443881Z","shell.execute_reply":"2022-09-10T10:22:32.449422Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"classes_la = {\"fruit_brownspot\": 1, \"fruit_healthy\": 2, \"fruit_woodiness\":3}\n\ntrain_df[\"class\"] = train_df[\"class\"].apply(lambda x: classes_la[x])","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.452833Z","iopub.execute_input":"2022-09-10T10:22:32.453671Z","iopub.status.idle":"2022-09-10T10:22:32.464500Z","shell.execute_reply.started":"2022-09-10T10:22:32.453640Z","shell.execute_reply":"2022-09-10T10:22:32.463258Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df = train_df.copy() # create a copy of the train df\n","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.468049Z","iopub.execute_input":"2022-09-10T10:22:32.470182Z","iopub.status.idle":"2022-09-10T10:22:32.476214Z","shell.execute_reply.started":"2022-09-10T10:22:32.470154Z","shell.execute_reply":"2022-09-10T10:22:32.475242Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.477931Z","iopub.execute_input":"2022-09-10T10:22:32.478909Z","iopub.status.idle":"2022-09-10T10:22:32.497301Z","shell.execute_reply.started":"2022-09-10T10:22:32.478882Z","shell.execute_reply":"2022-09-10T10:22:32.496232Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      Image_ID  class   xmin   ymin  width  height   xmax   ymax\n0  ID_007FAIEI      3   87.0   87.5  228.0   311.0  315.0  398.5\n1  ID_00G8K1V3      1   97.5   17.5  245.0   354.5  342.5  372.0\n2  ID_00WROUT9      1  156.5  209.5  248.0   302.5  404.5  512.0\n3  ID_00ZJEEK3      2  125.0  193.0  254.5   217.0  379.5  410.0\n4  ID_018UIENR      1   79.5  232.5  233.5   182.0  313.0  414.5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>width</th>\n      <th>height</th>\n      <th>xmax</th>\n      <th>ymax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_007FAIEI</td>\n      <td>3</td>\n      <td>87.0</td>\n      <td>87.5</td>\n      <td>228.0</td>\n      <td>311.0</td>\n      <td>315.0</td>\n      <td>398.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_00G8K1V3</td>\n      <td>1</td>\n      <td>97.5</td>\n      <td>17.5</td>\n      <td>245.0</td>\n      <td>354.5</td>\n      <td>342.5</td>\n      <td>372.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_00WROUT9</td>\n      <td>1</td>\n      <td>156.5</td>\n      <td>209.5</td>\n      <td>248.0</td>\n      <td>302.5</td>\n      <td>404.5</td>\n      <td>512.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_00ZJEEK3</td>\n      <td>2</td>\n      <td>125.0</td>\n      <td>193.0</td>\n      <td>254.5</td>\n      <td>217.0</td>\n      <td>379.5</td>\n      <td>410.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_018UIENR</td>\n      <td>1</td>\n      <td>79.5</td>\n      <td>232.5</td>\n      <td>233.5</td>\n      <td>182.0</td>\n      <td>313.0</td>\n      <td>414.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_grp = df.groupby(['Image_ID'])","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.501936Z","iopub.execute_input":"2022-09-10T10:22:32.502879Z","iopub.status.idle":"2022-09-10T10:22:32.508123Z","shell.execute_reply.started":"2022-09-10T10:22:32.502844Z","shell.execute_reply":"2022-09-10T10:22:32.507117Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class PassionFruit(object):\n    def __init__(self, df, IMG_DIR, transforms): \n        self.df = df\n        self.img_dir = IMG_DIR\n        self.image_ids = self.df['Image_ID'].unique().tolist()\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_values = self.df[self.df['Image_ID'] == image_id]\n        image = cv2.imread(str(self.img_dir)+str(image_id)+\".jpg\",cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        \n        boxes = image_values[['xmin', 'ymin', 'xmax', 'ymax']].to_numpy()\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        \n        labels = image_values[\"class\"].values\n        labels = torch.tensor(labels)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([idx])\n        target['area'] = torch.as_tensor(area, dtype=torch.float32)\n        target['iscrowd'] = torch.zeros(len(classes_la), dtype=torch.int64)\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n        \n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return torch.tensor(image), target, image_id","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.509809Z","iopub.execute_input":"2022-09-10T10:22:32.510460Z","iopub.status.idle":"2022-09-10T10:22:32.522962Z","shell.execute_reply.started":"2022-09-10T10:22:32.510408Z","shell.execute_reply":"2022-09-10T10:22:32.521898Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"pip install -U albumentations","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:32.526036Z","iopub.execute_input":"2022-09-10T10:22:32.527305Z","iopub.status.idle":"2022-09-10T10:22:42.771468Z","shell.execute_reply.started":"2022-09-10T10:22:32.527277Z","shell.execute_reply":"2022-09-10T10:22:42.770095Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (0.4.6)\nCollecting albumentations\n  Downloading albumentations-1.2.1-py3-none-any.whl (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m897.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.19.3)\nRequirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.0.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.7.3)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (6.0)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.5.4.60)\nRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (4.3.0)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (9.1.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\nRequirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.19.3)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.5)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->scikit-image>=0.16.1->albumentations) (5.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.9)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\nInstalling collected packages: albumentations\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 0.4.6\n    Uninstalling albumentations-0.4.6:\n      Successfully uninstalled albumentations-0.4.6\nSuccessfully installed albumentations-1.2.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        # A.HorizontalFlip(p=0.5),\n        # A.VerticalFlip(p=0.5),\n        A.RandomBrightness(),\n        A.RandomRotate90(),\n        A.Rotate(limit=(-90, 90)),\n        A.Transpose(),\n        A.Downscale (),\n        A.RandomContrast(),\n        A.RandomBrightnessContrast(),\n        A.RandomGamma(),\n        A.Blur(),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:42.773136Z","iopub.execute_input":"2022-09-10T10:22:42.775049Z","iopub.status.idle":"2022-09-10T10:22:42.783339Z","shell.execute_reply.started":"2022-09-10T10:22:42.775013Z","shell.execute_reply":"2022-09-10T10:22:42.782207Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"path=\"../input/makerere-passion-fruit-disease-detection-challenge/Train_Images/Train_Images/\"\npassion_dataset = PassionFruit(df, path, get_train_transform())","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:42.784588Z","iopub.execute_input":"2022-09-10T10:22:42.785664Z","iopub.status.idle":"2022-09-10T10:22:42.800473Z","shell.execute_reply.started":"2022-09-10T10:22:42.785628Z","shell.execute_reply":"2022-09-10T10:22:42.799432Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"image_ids = df['Image_ID'].unique()\nvalid_ids = image_ids[-665:]\ntrain_ids = image_ids[:-665]\nvalid_df = df[df['Image_ID'].isin(valid_ids)]\ntrain_df = df[df['Image_ID'].isin(train_ids)]\ntrain_df.shape,valid_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:42.802231Z","iopub.execute_input":"2022-09-10T10:22:42.803021Z","iopub.status.idle":"2022-09-10T10:22:42.818355Z","shell.execute_reply.started":"2022-09-10T10:22:42.802985Z","shell.execute_reply":"2022-09-10T10:22:42.817374Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"((3054, 8), (852, 8))"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = PassionFruit(df, path, get_train_transform())\nvalid_dataset = PassionFruit(df, path, get_valid_transform())\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=8,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=8,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:42.820276Z","iopub.execute_input":"2022-09-10T10:22:42.823290Z","iopub.status.idle":"2022-09-10T10:22:42.833312Z","shell.execute_reply.started":"2022-09-10T10:22:42.823262Z","shell.execute_reply":"2022-09-10T10:22:42.832333Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"num_classes = 4 # + background\n\n# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:42.835749Z","iopub.execute_input":"2022-09-10T10:22:42.837709Z","iopub.status.idle":"2022-09-10T10:22:51.427296Z","shell.execute_reply.started":"2022-09-10T10:22:42.837675Z","shell.execute_reply":"2022-09-10T10:22:51.426211Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/160M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7107d7c9962345d2a256acc65e14ed7f"}},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:51.428952Z","iopub.execute_input":"2022-09-10T10:22:51.429576Z","iopub.status.idle":"2022-09-10T10:22:51.497808Z","shell.execute_reply.started":"2022-09-10T10:22:51.429536Z","shell.execute_reply":"2022-09-10T10:22:51.494503Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.009, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:51.500979Z","iopub.execute_input":"2022-09-10T10:22:51.501300Z","iopub.status.idle":"2022-09-10T10:22:54.362637Z","shell.execute_reply.started":"2022-09-10T10:22:51.501270Z","shell.execute_reply":"2022-09-10T10:22:54.361655Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:54.363926Z","iopub.execute_input":"2022-09-10T10:22:54.364283Z","iopub.status.idle":"2022-09-10T10:22:54.369559Z","shell.execute_reply.started":"2022-09-10T10:22:54.364247Z","shell.execute_reply":"2022-09-10T10:22:54.368498Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import sys\nbest_epoch = 0\nmin_loss = sys.maxsize\n\nfor epoch in range(num_epochs):\n    tk = tqdm(train_data_loader)\n    model.train();\n    for images, targets, image_ids in tk:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        tk.set_postfix(train_loss=loss_value)\n    tk.close()\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    \n    print(f\"Epoch #{epoch} loss: {loss_value}\") \n        \n    #validation \n    model.eval();\n    with torch.no_grad():\n        tk = tqdm(valid_data_loader)\n        for images, targets, image_ids in tk:\n        \n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            val_output = model(images)\n            val_output = [{k: v.to(device) for k, v in t.items()} for t in val_output]\n            IOU = []\n            for j in range(len(val_output)):\n                a,b = val_output[j]['boxes'].cpu().detach(), targets[j]['boxes'].cpu().detach()\n                chk = torchvision.ops.box_iou(a,b)\n                res = np.nanmean(chk.sum(axis=1)/(chk>0).sum(axis=1))\n                IOU.append(res)\n            tk.set_postfix(IoU=np.mean(IOU))\n        tk.close()","metadata":{"execution":{"iopub.status.busy":"2022-09-10T10:22:54.370854Z","iopub.execute_input":"2022-09-10T10:22:54.371873Z","iopub.status.idle":"2022-09-10T13:15:49.175876Z","shell.execute_reply.started":"2022-09-10T10:22:54.371837Z","shell.execute_reply":"2022-09-10T13:15:49.174585Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c862b04ba5945ceabcba3365354752e"}},"metadata":{}},{"name":"stdout","text":"Epoch #0 loss: 0.12888084752690757\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04f14401e3b441eb85f6da035bbd538"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"877d621bfc414ef6a7c85a4cabc714cc"}},"metadata":{}},{"name":"stdout","text":"Epoch #1 loss: 0.09777241870463568\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"454e5abef1a4484eb6c4301b08c81d2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7fdf4b2079c4726b795bdb40c7437e5"}},"metadata":{}},{"name":"stdout","text":"Epoch #2 loss: 0.0851768027744736\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4d1a75f86084dc4aa5772b942623480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37752cbf84d34add8a782af802adea3a"}},"metadata":{}},{"name":"stdout","text":"Epoch #3 loss: 0.0910236684377402\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13945bd49745455583eccdd91b72db2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aadd908393654e8eae780b7602c0d30f"}},"metadata":{}},{"name":"stdout","text":"Epoch #4 loss: 0.09319095433248084\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f484c837d64b44b6b699df713d3872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e832bce1ef4722ab4838f61d2e71a1"}},"metadata":{}},{"name":"stdout","text":"Epoch #5 loss: 0.09443076101018853\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dbc32d1de0848d5a49cef580458cbb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa885b2afdc642ffab177adbe76f49d7"}},"metadata":{}},{"name":"stdout","text":"Epoch #6 loss: 0.08517530501019227\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a19b66eb09d4f61aae09ca26c344e5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73e4899b908b44ee96e19577051e260a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"833f54746c09417d9b2e14fe92e016ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7249064642b44bd95fcfe565af4cd75"}},"metadata":{}},{"name":"stdout","text":"Epoch #9 loss: 0.09522511911463152\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5e558137054895a288fb045473272d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"073cb40f4b7a4335ad71874ed06c837e"}},"metadata":{}},{"name":"stdout","text":"Epoch #10 loss: 0.09456511218880989\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64818058dcae4714a026723037b284e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815738a9dc4a46468e5888ad3cbae122"}},"metadata":{}},{"name":"stdout","text":"Epoch #11 loss: 0.06703697695213193\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87fe65fb793243d7bb590819d1e98b2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f03b13d8bd2d4d41bf134827cfa60a09"}},"metadata":{}},{"name":"stdout","text":"Epoch #12 loss: 0.08797789919434021\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3a04c53f1b54db084df409e00bc1866"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a3f4ffc640c45e8b118ba657ee0b8b0"}},"metadata":{}},{"name":"stdout","text":"Epoch #13 loss: 0.08087929554911388\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a781f9bfa5cd4704b47916fc62f6dc9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c08fc16601d4a1cbcd28b6e16e67992"}},"metadata":{}},{"name":"stdout","text":"Epoch #14 loss: 0.09021726217121563\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2856802df4e6483895dc462f49fe1112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31bdf1f7e060411a90e45dd0a1c3b91d"}},"metadata":{}},{"name":"stdout","text":"Epoch #15 loss: 0.08280909422250693\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6e28dce18ce492a9cbef3e05e31d58a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"599b12ef6640486992bd58c4c53c8439"}},"metadata":{}},{"name":"stdout","text":"Epoch #16 loss: 0.07536330018216919\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc6aa7011384a70a1dbfffbe91c9f21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90bf9f2f43254a8ab35d2a0d273e12cb"}},"metadata":{}},{"name":"stdout","text":"Epoch #17 loss: 0.09084910584128865\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d87fa9fe2c44b3a9c2b760ac92d5962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3155f1c7155436dadd6621d410c7c40"}},"metadata":{}},{"name":"stdout","text":"Epoch #18 loss: 0.09716965409186286\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76646004abbd40f1847041f0c16d3cd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe783f9cb4384b0a8488fe924c3975be"}},"metadata":{}},{"name":"stdout","text":"Epoch #19 loss: 0.08274986312082248\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cb2e1f5732b4fe997a0d084b36ad8db"}},"metadata":{}}]},{"cell_type":"code","source":"img,target,_ = valid_dataset[5]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['boxes']))\nprint('real #boxes: ', len(target['boxes']))","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:15:49.177790Z","iopub.execute_input":"2022-09-10T13:15:49.178199Z","iopub.status.idle":"2022-09-10T13:15:49.253586Z","shell.execute_reply.started":"2022-09-10T13:15:49.178160Z","shell.execute_reply":"2022-09-10T13:15:49.252502Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"predicted #boxes:  1\nreal #boxes:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/makerere-passion-fruit-disease-detection-challenge/Test (12).csv\")","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:15:49.254797Z","iopub.execute_input":"2022-09-10T13:15:49.255134Z","iopub.status.idle":"2022-09-10T13:15:49.264059Z","shell.execute_reply.started":"2022-09-10T13:15:49.255100Z","shell.execute_reply":"2022-09-10T13:15:49.263029Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class TestDataset(object):\n    def __init__(self, df, IMG_DIR, transforms):        \n        self.df = df\n        self.img_dir = IMG_DIR\n        self.transforms = transforms\n        self.image_ids = self.df['Image_ID'].tolist()\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):        \n        image_id = self.image_ids[idx]\n        image = cv2.imread(self.img_dir+image_id+\".jpg\",cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:15:49.265569Z","iopub.execute_input":"2022-09-10T13:15:49.266117Z","iopub.status.idle":"2022-09-10T13:15:49.275112Z","shell.execute_reply.started":"2022-09-10T13:15:49.266077Z","shell.execute_reply":"2022-09-10T13:15:49.273871Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def get_test_transform(IMG_SIZE=(512,512)):\n    return A.Compose([\n         A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n        A.Resize(*IMG_SIZE),\n        ToTensorV2(p=1.0)\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:15:49.276747Z","iopub.execute_input":"2022-09-10T13:15:49.277199Z","iopub.status.idle":"2022-09-10T13:15:49.285845Z","shell.execute_reply.started":"2022-09-10T13:15:49.277164Z","shell.execute_reply":"2022-09-10T13:15:49.284839Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"test_img_dir = \"../input/makerere-passion-fruit-disease-detection-challenge/Test_Images (1)/Test_Images/\"","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:15:49.287212Z","iopub.execute_input":"2022-09-10T13:15:49.287594Z","iopub.status.idle":"2022-09-10T13:15:49.296351Z","shell.execute_reply.started":"2022-09-10T13:15:49.287560Z","shell.execute_reply":"2022-09-10T13:15:49.295364Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = (512,512)\ntest_dataset = TestDataset(submission, test_img_dir ,get_test_transform())","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:15:49.298957Z","iopub.execute_input":"2022-09-10T13:15:49.299224Z","iopub.status.idle":"2022-09-10T13:15:49.307732Z","shell.execute_reply.started":"2022-09-10T13:15:49.299200Z","shell.execute_reply":"2022-09-10T13:15:49.306634Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"results = []\nfor j in range(submission.shape[0]):\n    \n    img,_ = test_dataset[j]\n    img = img.unsqueeze_(0)\n    # put the model in evaluation mode\n    model.eval()\n\n    with torch.no_grad():\n        prediction = model([img.to(device)][0])\n        aa = zip(prediction[0][\"boxes\"].tolist(), prediction[0][\"labels\"].tolist(), prediction[0][\"scores\"].tolist())\n       \n        for item in list(aa):\n            row_dict = {}\n            row_dict[\"Image_ID\"] = _\n            row_dict[\"boxes\"] = item[0]\n            row_dict[\"labels\"] = item[1]\n            row_dict[\"confidence\"] = item[2]\n            results.append(row_dict)\nsub_df = pd.DataFrame(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:15:49.315936Z","iopub.execute_input":"2022-09-10T13:15:49.316194Z","iopub.status.idle":"2022-09-10T13:16:59.585845Z","shell.execute_reply.started":"2022-09-10T13:15:49.316171Z","shell.execute_reply":"2022-09-10T13:16:59.584506Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"sub_df[\"ymin\"] = sub_df[\"boxes\"].apply(lambda x: x[1])\nsub_df[\"xmin\"] = sub_df[\"boxes\"].apply(lambda x: x[0])\nsub_df[\"ymax\"] = sub_df[\"boxes\"].apply(lambda x: x[3])\nsub_df[\"xmax\"]=  sub_df[\"boxes\"].apply(lambda x: x[2])\nclasses_la = {0:\"Background\", 1:\"fruit_brownspot\", 2:\"fruit_healthy\", 3:\"fruit_woodiness\"}\nsub_df[\"labels\"] = sub_df[\"labels\"].apply(lambda x: classes_la[x])\nsub_df.drop([\"boxes\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:16:59.590835Z","iopub.execute_input":"2022-09-10T13:16:59.593703Z","iopub.status.idle":"2022-09-10T13:16:59.618816Z","shell.execute_reply.started":"2022-09-10T13:16:59.593649Z","shell.execute_reply":"2022-09-10T13:16:59.617797Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"sub_df.rename(columns={\"labels\":\"class\"}, inplace=True)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:16:59.623106Z","iopub.execute_input":"2022-09-10T13:16:59.625807Z","iopub.status.idle":"2022-09-10T13:16:59.648336Z","shell.execute_reply.started":"2022-09-10T13:16:59.625766Z","shell.execute_reply":"2022-09-10T13:16:59.647453Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"      Image_ID            class  confidence        ymin        xmin  \\\n0  ID_IUJJG62B    fruit_healthy    0.995625  130.676331   62.407578   \n1  ID_IUJJG62B    fruit_healthy    0.976740  308.390564  284.998718   \n2  ID_IUJJG62B    fruit_healthy    0.918132  400.072205  436.283203   \n3  ID_IUJJG62B    fruit_healthy    0.362619  351.239410  415.790619   \n4  ID_IUJJG62B  fruit_woodiness    0.300863  321.679810  291.971313   \n\n         ymax        xmax  \n0  408.990875  387.189056  \n1  482.697113  490.715881  \n2  487.800354  512.000000  \n3  484.796967  508.485138  \n4  478.376434  481.197876  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>confidence</th>\n      <th>ymin</th>\n      <th>xmin</th>\n      <th>ymax</th>\n      <th>xmax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_IUJJG62B</td>\n      <td>fruit_healthy</td>\n      <td>0.995625</td>\n      <td>130.676331</td>\n      <td>62.407578</td>\n      <td>408.990875</td>\n      <td>387.189056</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_IUJJG62B</td>\n      <td>fruit_healthy</td>\n      <td>0.976740</td>\n      <td>308.390564</td>\n      <td>284.998718</td>\n      <td>482.697113</td>\n      <td>490.715881</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_IUJJG62B</td>\n      <td>fruit_healthy</td>\n      <td>0.918132</td>\n      <td>400.072205</td>\n      <td>436.283203</td>\n      <td>487.800354</td>\n      <td>512.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_IUJJG62B</td>\n      <td>fruit_healthy</td>\n      <td>0.362619</td>\n      <td>351.239410</td>\n      <td>415.790619</td>\n      <td>484.796967</td>\n      <td>508.485138</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_IUJJG62B</td>\n      <td>fruit_woodiness</td>\n      <td>0.300863</td>\n      <td>321.679810</td>\n      <td>291.971313</td>\n      <td>478.376434</td>\n      <td>481.197876</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sub_df.to_csv(\"Submission_20fgsdfg.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-10T13:16:59.652318Z","iopub.execute_input":"2022-09-10T13:16:59.654949Z","iopub.status.idle":"2022-09-10T13:16:59.717472Z","shell.execute_reply.started":"2022-09-10T13:16:59.654908Z","shell.execute_reply":"2022-09-10T13:16:59.716306Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}