{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport random\nimport numpy as np\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import (\n    AdamW,\n    Trainer,\n    TrainingArguments,\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_cosine_schedule_with_warmup,\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-31T13:50:53.869302Z","iopub.execute_input":"2022-08-31T13:50:53.870463Z","iopub.status.idle":"2022-08-31T13:50:53.878573Z","shell.execute_reply.started":"2022-08-31T13:50:53.870409Z","shell.execute_reply":"2022-08-31T13:50:53.876151Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model_name = \"roberta-large\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, ignore_mismatched_sizes=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:53.881944Z","iopub.execute_input":"2022-08-31T13:50:53.882414Z","iopub.status.idle":"2022-08-31T13:50:57.334794Z","shell.execute_reply.started":"2022-08-31T13:50:53.882366Z","shell.execute_reply":"2022-08-31T13:50:57.333639Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Could not locate the tokenizer configuration file, will try to use the model config instead.\nloading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\nModel config RobertaConfig {\n  \"_name_or_path\": \"roberta-large\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\nloading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\nloading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\nloading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\nloading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\nModel config RobertaConfig {\n  \"_name_or_path\": \"roberta-large\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"lr = 2e-5\nepochs =  6\nbatch_size = 5\nmax_seq_len = 75\n\ntest_frac = 0.1","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.337335Z","iopub.execute_input":"2022-08-31T13:50:57.337768Z","iopub.status.idle":"2022-08-31T13:50:57.343253Z","shell.execute_reply.started":"2022-08-31T13:50:57.337729Z","shell.execute_reply":"2022-08-31T13:50:57.342077Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef set_seed(seed=106052):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.345100Z","iopub.execute_input":"2022-08-31T13:50:57.345562Z","iopub.status.idle":"2022-08-31T13:50:57.355578Z","shell.execute_reply.started":"2022-08-31T13:50:57.345487Z","shell.execute_reply":"2022-08-31T13:50:57.354424Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class CEFRDataset(Dataset):\n    \"\"\"Classification dataset, built on top of pytorch dataset object\n    \"\"\"\n    \n    def __init__(self, texts, labels):\n        \n        self.encoder = LabelEncoder()\n        print(self.encoder.__dict__)\n        self.texts = texts\n        self.labels = self.encoder.fit_transform(labels)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        label = self.labels[index]\n        encoded_text = tokenizer(\n            text,\n            padding=\"max_length\",\n            max_length=max_seq_len,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        encoded_text[\"input_ids\"] = encoded_text[\"input_ids\"].squeeze()\n        encoded_text[\"attention_mask\"] = encoded_text[\"attention_mask\"].squeeze()\n        label = torch.tensor(label)\n\n        return {\n            \"input_ids\": encoded_text[\"input_ids\"],\n            \"attention_mask\": encoded_text[\"attention_mask\"],\n            \"labels\": label,\n        }\n\n    def get_labels(self):\n        return self.labels","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.359942Z","iopub.execute_input":"2022-08-31T13:50:57.360293Z","iopub.status.idle":"2022-08-31T13:50:57.372109Z","shell.execute_reply.started":"2022-08-31T13:50:57.360265Z","shell.execute_reply":"2022-08-31T13:50:57.371165Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def train(train_set, valid_set, epochs=5, warmup_size=0.1, lr=1e-3, batch_size=1):\n    model = get_model(model_name)\n    optim = AdamW(model.parameters(), lr=lr)\n    scheduler = get_scheduler(\n        optim, warmup_size, round(len(train_set) / batch_size * epochs)\n    )\n    training_args = get_training_args(epochs, batch_size)\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_set,\n        eval_dataset=valid_set,\n        optimizers=[optim, scheduler],\n        compute_metrics=compute_accuracy\n    )\n    trainer.train()\n    trainer.save_model()\n    return trainer","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.373441Z","iopub.execute_input":"2022-08-31T13:50:57.373955Z","iopub.status.idle":"2022-08-31T13:50:57.383831Z","shell.execute_reply.started":"2022-08-31T13:50:57.373842Z","shell.execute_reply":"2022-08-31T13:50:57.382802Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def get_model(pretrained_checkpoint):\n    model = AutoModelForSequenceClassification.from_pretrained(\n        pretrained_checkpoint, num_labels=2, ignore_mismatched_sizes=True\n    )\n    return model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.385382Z","iopub.execute_input":"2022-08-31T13:50:57.386580Z","iopub.status.idle":"2022-08-31T13:50:57.395622Z","shell.execute_reply.started":"2022-08-31T13:50:57.386541Z","shell.execute_reply":"2022-08-31T13:50:57.394610Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\ndef get_scheduler(optimizer, warmup_size, total_steps):\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=round(total_steps * warmup_size),\n        num_training_steps=total_steps,\n    )\n    return scheduler\n\n\ndef get_training_args(epochs, batch_size):\n    return TrainingArguments(\n        output_dir=\"./b\",\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        logging_steps=50,\n        fp16=False,\n        evaluation_strategy=\"epoch\",\n        eval_accumulation_steps=1,\n        report_to=None,\n#         save_total_limit=1,\n#         load_best_model_at_end=True,\n        save_strategy = 'epoch'\n    )\n\n\ndef compute_accuracy(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc}\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.397298Z","iopub.execute_input":"2022-08-31T13:50:57.397912Z","iopub.status.idle":"2022-08-31T13:50:57.411870Z","shell.execute_reply.started":"2022-08-31T13:50:57.397877Z","shell.execute_reply":"2022-08-31T13:50:57.410856Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"lr = 2e-5\nepochs =  8\nbatch_size = 8\nmax_seq_len = 512","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.413281Z","iopub.execute_input":"2022-08-31T13:50:57.414619Z","iopub.status.idle":"2022-08-31T13:50:57.423296Z","shell.execute_reply.started":"2022-08-31T13:50:57.414580Z","shell.execute_reply":"2022-08-31T13:50:57.422173Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def split_valid(df, frac=0.1):\n    \n    val = pd.DataFrame()\n    val[\"text\"] = \"\"\n    val[\"label\"] = -1\n    \n    for i in df.label.unique():\n        val = pd.concat([val, df[df.label == i].sample(frac=frac)])\n        \n    return df[~df.index.isin(val.index)].reset_index(drop=True) , val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.425047Z","iopub.execute_input":"2022-08-31T13:50:57.425776Z","iopub.status.idle":"2022-08-31T13:50:57.435681Z","shell.execute_reply.started":"2022-08-31T13:50:57.425741Z","shell.execute_reply":"2022-08-31T13:50:57.434623Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_set_df = pd.read_csv(\"../input/covid19-tweet-classification-challenge-by-zindi/updated_train.csv\")\ntrain_set_df.drop(\"ID\", axis=1, inplace=True)\ntrain_set_df = train_set_df.reset_index(drop=True)\ntrain_set_df.columns=[\"text\",\"label\"]\ntrain_set_df = train_set_df[train_set_df.label != \"-\"]\ntrain_set_df = train_set_df[[\"text\", \"label\"]]","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.440221Z","iopub.execute_input":"2022-08-31T13:50:57.441551Z","iopub.status.idle":"2022-08-31T13:50:57.466533Z","shell.execute_reply.started":"2022-08-31T13:50:57.441515Z","shell.execute_reply":"2022-08-31T13:50:57.465692Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_set_df.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.467983Z","iopub.execute_input":"2022-08-31T13:50:57.468621Z","iopub.status.idle":"2022-08-31T13:50:57.477539Z","shell.execute_reply.started":"2022-08-31T13:50:57.468585Z","shell.execute_reply":"2022-08-31T13:50:57.476555Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0    2746\n1    2541\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\ntrain_set_df.text = train_set_df.text.apply(lambda x: x.replace(\"\\r\", \"\").replace(\"\\n\", \" \"))\n\n# extra_df = pd.read_csv(\"../input/frenchcefr/french_mike_june.csv\")\n# extra_df.columns = [\"text\", \"label\", \"label_\"]\n# extra_df = extra_df[[\"text\", \"label\"]]\n# extra_df.text = extra_df.text.astype(str)\n#train_set_df = pd.concat([train_set_df, extra_df]).reset_index(drop=True)\n\ntrain_set_df, valid_set_df = split_valid(train_set_df)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.479148Z","iopub.execute_input":"2022-08-31T13:50:57.480303Z","iopub.status.idle":"2022-08-31T13:50:57.497073Z","shell.execute_reply.started":"2022-08-31T13:50:57.480265Z","shell.execute_reply":"2022-08-31T13:50:57.495974Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train_set_df.label = le.fit_transform(train_set_df.label)\nvalid_set_df.label = le.transform(valid_set_df.label)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.498630Z","iopub.execute_input":"2022-08-31T13:50:57.499248Z","iopub.status.idle":"2022-08-31T13:50:57.508771Z","shell.execute_reply.started":"2022-08-31T13:50:57.499207Z","shell.execute_reply":"2022-08-31T13:50:57.507844Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"valid_set_df","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.512069Z","iopub.execute_input":"2022-08-31T13:50:57.513215Z","iopub.status.idle":"2022-08-31T13:50:57.527316Z","shell.execute_reply.started":"2022-08-31T13:50:57.513177Z","shell.execute_reply":"2022-08-31T13:50:57.526206Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                                  text  label\n0    All empty during a show at the Max Stadium bef...      1\n1    ki saru 41 donated 2cr to mumbai police in thi...      1\n2    Soft water soft skin Looking for water softene...      1\n3    Hospitals get paid more if patients listed as ...      1\n4    Italian politician Vittorio Sgarbi reports in ...      1\n..                                                 ...    ...\n524  Stephon Marbury says Larry Brown tried to kick...      0\n525  He left it on the table Unfinished All the spo...      0\n526  In these tough times the best way to grow is t...      0\n527  poole Lib Dem Leader Fasts for Holy Ramadan in...      0\n528  Italian Prime Minister Giuseppe Conte gave the...      0\n\n[529 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>All empty during a show at the Max Stadium bef...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ki saru 41 donated 2cr to mumbai police in thi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Soft water soft skin Looking for water softene...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hospitals get paid more if patients listed as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Italian politician Vittorio Sgarbi reports in ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>524</th>\n      <td>Stephon Marbury says Larry Brown tried to kick...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>525</th>\n      <td>He left it on the table Unfinished All the spo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>526</th>\n      <td>In these tough times the best way to grow is t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>527</th>\n      <td>poole Lib Dem Leader Fasts for Holy Ramadan in...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>528</th>\n      <td>Italian Prime Minister Giuseppe Conte gave the...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>529 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm \n\ndef predict(model, text):\n    \n    preds = []\n    \n    for i in tqdm(range(len(text))):\n        tokenized = tokenizer(text[i:i+1], return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n        pred = model(**tokenized)\n        preds.append(pred.logits.argmax(-1).item())\n\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.528974Z","iopub.execute_input":"2022-08-31T13:50:57.529591Z","iopub.status.idle":"2022-08-31T13:50:57.538478Z","shell.execute_reply.started":"2022-08-31T13:50:57.529550Z","shell.execute_reply":"2022-08-31T13:50:57.537302Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:50:57.540323Z","iopub.execute_input":"2022-08-31T13:50:57.541039Z","iopub.status.idle":"2022-08-31T13:50:57.743747Z","shell.execute_reply.started":"2022-08-31T13:50:57.541001Z","shell.execute_reply":"2022-08-31T13:50:57.742682Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"69"},"metadata":{}}]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:53:46.589883Z","iopub.execute_input":"2022-08-31T13:53:46.590304Z","iopub.status.idle":"2022-08-31T13:53:46.617743Z","shell.execute_reply.started":"2022-08-31T13:53:46.590267Z","shell.execute_reply":"2022-08-31T13:53:46.616470Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"train_set_df = reduce_mem_usage(train_set_df)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:53:48.181445Z","iopub.execute_input":"2022-08-31T13:53:48.181948Z","iopub.status.idle":"2022-08-31T13:53:48.196056Z","shell.execute_reply.started":"2022-08-31T13:53:48.181898Z","shell.execute_reply":"2022-08-31T13:53:48.194376Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Memory usage after optimization is: 0.04 MB\nDecreased by 43.7%\n","output_type":"stream"}]},{"cell_type":"code","source":"export PYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:2","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:55:48.738073Z","iopub.execute_input":"2022-08-31T13:55:48.738510Z","iopub.status.idle":"2022-08-31T13:55:48.746146Z","shell.execute_reply.started":"2022-08-31T13:55:48.738477Z","shell.execute_reply":"2022-08-31T13:55:48.744454Z"},"trusted":true},"execution_count":44,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_17/645615635.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    export PYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:2\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (645615635.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"train_set = CEFRDataset(train_set_df[\"text\"], train_set_df[\"label\"])\nvalid_set = CEFRDataset(valid_set_df[\"text\"], valid_set_df[\"label\"])\n\n\ntrainer_second = train(train_set, valid_set, epochs=epochs, warmup_size=0.2, lr=lr, batch_size=batch_size)\nmodel = trainer_second.model","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:55:49.059545Z","iopub.execute_input":"2022-08-31T13:55:49.059998Z","iopub.status.idle":"2022-08-31T13:55:55.300996Z","shell.execute_reply.started":"2022-08-31T13:55:49.059961Z","shell.execute_reply":"2022-08-31T13:55:55.299313Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"{}\n{}\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\nModel config RobertaConfig {\n  \"_name_or_path\": \"roberta-large\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3888882614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_second\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/18655252.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_set, valid_set, epochs, warmup_size, lr, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     scheduler = get_scheduler(\n\u001b[1;32m      5\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/3156865084.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(pretrained_checkpoint)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mpretrained_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     )\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def register_backward_hook(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 15.90 GiB total capacity; 15.05 GiB already allocated; 31.75 MiB free; 15.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 15.90 GiB total capacity; 15.05 GiB already allocated; 31.75 MiB free; 15.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"class CEFRDatasettest(Dataset):\n    \"\"\"Classification dataset, built on top of pytorch dataset object\n    \"\"\"\n    \n    def __init__(self, texts):\n        \n        self.encoder = LabelEncoder()\n        print(self.encoder.__dict__)\n        self.texts = texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        encoded_text = tokenizer(\n            text,\n            padding=\"max_length\",\n            max_length=max_seq_len,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        encoded_text[\"input_ids\"] = encoded_text[\"input_ids\"].squeeze()\n        encoded_text[\"attention_mask\"] = encoded_text[\"attention_mask\"].squeeze()\n\n        return {\n            \"input_ids\": encoded_text[\"input_ids\"],\n            \"attention_mask\": encoded_text[\"attention_mask\"],\n        }\n\n    def get_labels(self):\n        return self.labels","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-08-31T13:51:04.397820Z","iopub.status.idle":"2022-08-31T13:51:04.398457Z","shell.execute_reply.started":"2022-08-31T13:51:04.398149Z","shell.execute_reply":"2022-08-31T13:51:04.398175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/covid19-tweet-classification-challenge-by-zindi/updated_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:51:04.400977Z","iopub.status.idle":"2022-08-31T13:51:04.401500Z","shell.execute_reply.started":"2022-08-31T13:51:04.401224Z","shell.execute_reply":"2022-08-31T13:51:04.401248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds=predict(model, test.text.to_list())","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:51:04.403546Z","iopub.status.idle":"2022-08-31T13:51:04.404051Z","shell.execute_reply.started":"2022-08-31T13:51:04.403789Z","shell.execute_reply":"2022-08-31T13:51:04.403813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv(\"../input/covid19-tweet-classification-challenge-by-zindi/updated_ss.csv\")\nsub.target=preds\nsub","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:51:04.406115Z","iopub.status.idle":"2022-08-31T13:51:04.406633Z","shell.execute_reply.started":"2022-08-31T13:51:04.406354Z","shell.execute_reply":"2022-08-31T13:51:04.406377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\ndef create_submission(submission_file, submission_name):\n    submission_file.to_csv(submission_name+\".csv\",index=False)\n    return FileLink(submission_name+\".csv\")\ncreate_submission(sub, \"submission\")","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:51:04.407977Z","iopub.status.idle":"2022-08-31T13:51:04.409247Z","shell.execute_reply.started":"2022-08-31T13:51:04.408937Z","shell.execute_reply":"2022-08-31T13:51:04.408966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****","metadata":{}}]}