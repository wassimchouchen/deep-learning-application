{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport cv2\nimport ast\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\n\nimport random\n\nimport torch\n\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-11T09:57:55.865222Z","iopub.execute_input":"2022-09-11T09:57:55.866145Z","iopub.status.idle":"2022-09-11T09:57:58.050442Z","shell.execute_reply.started":"2022-09-11T09:57:55.866009Z","shell.execute_reply":"2022-09-11T09:57:58.049448Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install albumentations==0.4.6\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:57:58.052483Z","iopub.execute_input":"2022-09-11T09:57:58.053070Z","iopub.status.idle":"2022-09-11T09:58:12.171866Z","shell.execute_reply.started":"2022-09-11T09:57:58.053027Z","shell.execute_reply":"2022-09-11T09:58:12.170829Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting albumentations==0.4.6\n  Downloading albumentations-0.4.6.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m767.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (1.7.3)\nRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (6.0)\nRequirement already satisfied: opencv-python>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==0.4.6) (4.5.4.60)\nRequirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.19.3)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.0)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.19.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (9.1.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.5.3)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (21.3)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.33.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.3.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (5.1.1)\nBuilding wheels for collected packages: albumentations\n  Building wheel for albumentations (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=9be653211e3e53b3fab7aa08a9ed20d065c905dce4ccba9c42a4fa7a32a90ab8\n  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\nSuccessfully built albumentations\nInstalling collected packages: albumentations\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.2.1\n    Uninstalling albumentations-1.2.1:\n      Successfully uninstalled albumentations-1.2.1\nSuccessfully installed albumentations-0.4.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/makerere-passion-fruit-disease-detection-challenge/Train (11).csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.173409Z","iopub.execute_input":"2022-09-11T09:58:12.173775Z","iopub.status.idle":"2022-09-11T09:58:12.208686Z","shell.execute_reply.started":"2022-09-11T09:58:12.173736Z","shell.execute_reply":"2022-09-11T09:58:12.207705Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      Image_ID            class   xmin   ymin  width  height\n0  ID_007FAIEI  fruit_woodiness   87.0   87.5  228.0   311.0\n1  ID_00G8K1V3  fruit_brownspot   97.5   17.5  245.0   354.5\n2  ID_00WROUT9  fruit_brownspot  156.5  209.5  248.0   302.5\n3  ID_00ZJEEK3    fruit_healthy  125.0  193.0  254.5   217.0\n4  ID_018UIENR  fruit_brownspot   79.5  232.5  233.5   182.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>width</th>\n      <th>height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_007FAIEI</td>\n      <td>fruit_woodiness</td>\n      <td>87.0</td>\n      <td>87.5</td>\n      <td>228.0</td>\n      <td>311.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_00G8K1V3</td>\n      <td>fruit_brownspot</td>\n      <td>97.5</td>\n      <td>17.5</td>\n      <td>245.0</td>\n      <td>354.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_00WROUT9</td>\n      <td>fruit_brownspot</td>\n      <td>156.5</td>\n      <td>209.5</td>\n      <td>248.0</td>\n      <td>302.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_00ZJEEK3</td>\n      <td>fruit_healthy</td>\n      <td>125.0</td>\n      <td>193.0</td>\n      <td>254.5</td>\n      <td>217.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_018UIENR</td>\n      <td>fruit_brownspot</td>\n      <td>79.5</td>\n      <td>232.5</td>\n      <td>233.5</td>\n      <td>182.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"No_duplicates = train_df.drop_duplicates(subset=\"Image_ID\")\nprint(No_duplicates.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.211442Z","iopub.execute_input":"2022-09-11T09:58:12.211812Z","iopub.status.idle":"2022-09-11T09:58:12.228255Z","shell.execute_reply.started":"2022-09-11T09:58:12.211774Z","shell.execute_reply":"2022-09-11T09:58:12.227142Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(3001, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/makerere-passion-fruit-disease-detection-challenge/Test (12).csv\")\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.230355Z","iopub.execute_input":"2022-09-11T09:58:12.231089Z","iopub.status.idle":"2022-09-11T09:58:12.246852Z","shell.execute_reply.started":"2022-09-11T09:58:12.231054Z","shell.execute_reply":"2022-09-11T09:58:12.246011Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      Image_ID\n0  ID_IUJJG62B\n1  ID_ZPNDRD4T\n2  ID_AHFYB64P\n3  ID_L8JZLNTF\n4  ID_IFMUXGPL","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_IUJJG62B</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_ZPNDRD4T</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_AHFYB64P</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_L8JZLNTF</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_IFMUXGPL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df['xmax'] = train_df['xmin']+train_df['width']\ntrain_df['ymax'] = train_df['ymin']+train_df['height']","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.249828Z","iopub.execute_input":"2022-09-11T09:58:12.250361Z","iopub.status.idle":"2022-09-11T09:58:12.259909Z","shell.execute_reply.started":"2022-09-11T09:58:12.250334Z","shell.execute_reply":"2022-09-11T09:58:12.258950Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"classes_la = {\"fruit_brownspot\": 1, \"fruit_healthy\": 2, \"fruit_woodiness\":3}\n\ntrain_df[\"class\"] = train_df[\"class\"].apply(lambda x: classes_la[x])","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.263327Z","iopub.execute_input":"2022-09-11T09:58:12.265499Z","iopub.status.idle":"2022-09-11T09:58:12.273189Z","shell.execute_reply.started":"2022-09-11T09:58:12.265383Z","shell.execute_reply":"2022-09-11T09:58:12.272284Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df = train_df.copy() # create a copy of the train df\n","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.274676Z","iopub.execute_input":"2022-09-11T09:58:12.275069Z","iopub.status.idle":"2022-09-11T09:58:12.284432Z","shell.execute_reply.started":"2022-09-11T09:58:12.275035Z","shell.execute_reply":"2022-09-11T09:58:12.283443Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.288960Z","iopub.execute_input":"2022-09-11T09:58:12.289687Z","iopub.status.idle":"2022-09-11T09:58:12.305817Z","shell.execute_reply.started":"2022-09-11T09:58:12.289631Z","shell.execute_reply":"2022-09-11T09:58:12.304884Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      Image_ID  class   xmin   ymin  width  height   xmax   ymax\n0  ID_007FAIEI      3   87.0   87.5  228.0   311.0  315.0  398.5\n1  ID_00G8K1V3      1   97.5   17.5  245.0   354.5  342.5  372.0\n2  ID_00WROUT9      1  156.5  209.5  248.0   302.5  404.5  512.0\n3  ID_00ZJEEK3      2  125.0  193.0  254.5   217.0  379.5  410.0\n4  ID_018UIENR      1   79.5  232.5  233.5   182.0  313.0  414.5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>width</th>\n      <th>height</th>\n      <th>xmax</th>\n      <th>ymax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_007FAIEI</td>\n      <td>3</td>\n      <td>87.0</td>\n      <td>87.5</td>\n      <td>228.0</td>\n      <td>311.0</td>\n      <td>315.0</td>\n      <td>398.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_00G8K1V3</td>\n      <td>1</td>\n      <td>97.5</td>\n      <td>17.5</td>\n      <td>245.0</td>\n      <td>354.5</td>\n      <td>342.5</td>\n      <td>372.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_00WROUT9</td>\n      <td>1</td>\n      <td>156.5</td>\n      <td>209.5</td>\n      <td>248.0</td>\n      <td>302.5</td>\n      <td>404.5</td>\n      <td>512.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_00ZJEEK3</td>\n      <td>2</td>\n      <td>125.0</td>\n      <td>193.0</td>\n      <td>254.5</td>\n      <td>217.0</td>\n      <td>379.5</td>\n      <td>410.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_018UIENR</td>\n      <td>1</td>\n      <td>79.5</td>\n      <td>232.5</td>\n      <td>233.5</td>\n      <td>182.0</td>\n      <td>313.0</td>\n      <td>414.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_grp = df.groupby(['Image_ID'])","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.310500Z","iopub.execute_input":"2022-09-11T09:58:12.311146Z","iopub.status.idle":"2022-09-11T09:58:12.316353Z","shell.execute_reply.started":"2022-09-11T09:58:12.311094Z","shell.execute_reply":"2022-09-11T09:58:12.315144Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class PassionFruit(object):\n    def __init__(self, df, IMG_DIR, transforms): \n        self.df = df\n        self.img_dir = IMG_DIR\n        self.image_ids = self.df['Image_ID'].unique().tolist()\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_values = self.df[self.df['Image_ID'] == image_id]\n        image = cv2.imread(str(self.img_dir)+str(image_id)+\".jpg\",cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        \n        boxes = image_values[['xmin', 'ymin', 'xmax', 'ymax']].to_numpy()\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        \n        labels = image_values[\"class\"].values\n        labels = torch.tensor(labels)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([idx])\n        target['area'] = torch.as_tensor(area, dtype=torch.float32)\n        target['iscrowd'] = torch.zeros(len(classes_la), dtype=torch.int64)\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n        \n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return torch.tensor(image), target, image_id","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.318358Z","iopub.execute_input":"2022-09-11T09:58:12.319045Z","iopub.status.idle":"2022-09-11T09:58:12.331420Z","shell.execute_reply.started":"2022-09-11T09:58:12.319010Z","shell.execute_reply":"2022-09-11T09:58:12.330198Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"pip install -U albumentations","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:12.332970Z","iopub.execute_input":"2022-09-11T09:58:12.333837Z","iopub.status.idle":"2022-09-11T09:58:22.368424Z","shell.execute_reply.started":"2022-09-11T09:58:12.333797Z","shell.execute_reply":"2022-09-11T09:58:22.367277Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (0.4.6)\nCollecting albumentations\n  Downloading albumentations-1.2.1-py3-none-any.whl (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m792.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.21.6)\nRequirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.0.4)\nRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.19.3)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (6.0)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.5.4.60)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.7.3)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (4.3.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (9.1.1)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.5)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\nRequirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.19.3)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->scikit-image>=0.16.1->albumentations) (5.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.9)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\nInstalling collected packages: albumentations\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 0.4.6\n    Uninstalling albumentations-0.4.6:\n      Successfully uninstalled albumentations-0.4.6\nSuccessfully installed albumentations-1.2.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        # A.HorizontalFlip(p=0.5),\n        # A.VerticalFlip(p=0.5),\n        A.RandomBrightness(),\n        A.RandomRotate90(),\n        A.Rotate(limit=(-90, 90)),\n        A.Transpose(),\n        A.Downscale (),\n        A.RandomContrast(),\n        A.RandomBrightnessContrast(),\n        A.RandomGamma(),\n        A.Blur(),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:22.370296Z","iopub.execute_input":"2022-09-11T09:58:22.371057Z","iopub.status.idle":"2022-09-11T09:58:22.378784Z","shell.execute_reply.started":"2022-09-11T09:58:22.371005Z","shell.execute_reply":"2022-09-11T09:58:22.377582Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"path=\"../input/makerere-passion-fruit-disease-detection-challenge/Train_Images/Train_Images/\"\npassion_dataset = PassionFruit(df, path, get_train_transform())","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:22.380199Z","iopub.execute_input":"2022-09-11T09:58:22.381135Z","iopub.status.idle":"2022-09-11T09:58:22.396592Z","shell.execute_reply.started":"2022-09-11T09:58:22.381082Z","shell.execute_reply":"2022-09-11T09:58:22.395647Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"image_ids = df['Image_ID'].unique()\nvalid_ids = image_ids[-665:]\ntrain_ids = image_ids[:-665]\nvalid_df = df[df['Image_ID'].isin(valid_ids)]\ntrain_df = df[df['Image_ID'].isin(train_ids)]\ntrain_df.shape,valid_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:22.398306Z","iopub.execute_input":"2022-09-11T09:58:22.398686Z","iopub.status.idle":"2022-09-11T09:58:22.416023Z","shell.execute_reply.started":"2022-09-11T09:58:22.398621Z","shell.execute_reply":"2022-09-11T09:58:22.414946Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"((3054, 8), (852, 8))"},"metadata":{}}]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = PassionFruit(df, path, get_train_transform())\nvalid_dataset = PassionFruit(df, path, get_valid_transform())\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=8,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=8,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:22.417290Z","iopub.execute_input":"2022-09-11T09:58:22.418316Z","iopub.status.idle":"2022-09-11T09:58:22.428592Z","shell.execute_reply.started":"2022-09-11T09:58:22.418277Z","shell.execute_reply":"2022-09-11T09:58:22.427542Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"num_classes = 4 # + background\n\n# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:22.430287Z","iopub.execute_input":"2022-09-11T09:58:22.431190Z","iopub.status.idle":"2022-09-11T09:58:31.029955Z","shell.execute_reply.started":"2022-09-11T09:58:22.431148Z","shell.execute_reply":"2022-09-11T09:58:31.028919Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/160M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"803c272894ca401da2440b23f320ff24"}},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:31.031629Z","iopub.execute_input":"2022-09-11T09:58:31.032026Z","iopub.status.idle":"2022-09-11T09:58:31.100337Z","shell.execute_reply.started":"2022-09-11T09:58:31.031990Z","shell.execute_reply":"2022-09-11T09:58:31.098980Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(params, lr=0.009, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:31.102080Z","iopub.execute_input":"2022-09-11T09:58:31.103086Z","iopub.status.idle":"2022-09-11T09:58:34.030711Z","shell.execute_reply.started":"2022-09-11T09:58:31.103038Z","shell.execute_reply":"2022-09-11T09:58:34.029678Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:34.032176Z","iopub.execute_input":"2022-09-11T09:58:34.032550Z","iopub.status.idle":"2022-09-11T09:58:34.037647Z","shell.execute_reply.started":"2022-09-11T09:58:34.032514Z","shell.execute_reply":"2022-09-11T09:58:34.036679Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import sys\nbest_epoch = 0\nmin_loss = sys.maxsize\n\nfor epoch in range(num_epochs):\n    tk = tqdm(train_data_loader)\n    model.train();\n    for images, targets, image_ids in tk:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        tk.set_postfix(train_loss=loss_value)\n    tk.close()\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    \n    print(f\"Epoch #{epoch} loss: {loss_value}\") \n        \n    #validation \n    model.eval();\n    with torch.no_grad():\n        tk = tqdm(valid_data_loader)\n        for images, targets, image_ids in tk:\n        \n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            val_output = model(images)\n            val_output = [{k: v.to(device) for k, v in t.items()} for t in val_output]\n            IOU = []\n            for j in range(len(val_output)):\n                a,b = val_output[j]['boxes'].cpu().detach(), targets[j]['boxes'].cpu().detach()\n                chk = torchvision.ops.box_iou(a,b)\n                res = np.nanmean(chk.sum(axis=1)/(chk>0).sum(axis=1))\n                IOU.append(res)\n            tk.set_postfix(IoU=np.mean(IOU))\n        tk.close()","metadata":{"execution":{"iopub.status.busy":"2022-09-11T09:58:34.039122Z","iopub.execute_input":"2022-09-11T09:58:34.039958Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d76000ad1e0b400eb7b3ee45f7f88904"}},"metadata":{}},{"name":"stdout","text":"Epoch #0 loss: 2.1681633283566423e+30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b30c6a34ffe4e83ad1509b8908c61c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99152d3b36548259e1e721e3e7d511c"}},"metadata":{}},{"name":"stdout","text":"Epoch #1 loss: 1.6890044114976247e+20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97bf6ca99c0f4af3acd1195274456017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45ba2eb9b68b426d87fa5ea10b0b75b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ab39777070413abe78fefd162acbe5"}},"metadata":{}},{"name":"stdout","text":"Epoch #3 loss: 109629398283406.22\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeccac9845cb4c25a8e33351fde26748"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5015f74ea303435a8a6bb07c146dec41"}},"metadata":{}},{"name":"stdout","text":"Epoch #4 loss: 65260424705285.33\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"043adf3ffcb94d7c813a739e485979d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c36778d6e6340cf86cd68607d1b5dfa"}},"metadata":{}},{"name":"stdout","text":"Epoch #5 loss: 24130949573383.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2f23d2d2b444d5853a302ec21e86f8"}},"metadata":{}},{"name":"stdout","text":"Epoch #6 loss: 20538160978815.996\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0a98e8c4e3d420d93b5c6fe3b42a52c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f03be66ed74c239d483f3cf2d1d6df"}},"metadata":{}},{"name":"stdout","text":"Epoch #7 loss: 20472007634762.773\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e33839a36f48b99998d3e8fe78e87b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d121ad5bcb154b26b4d596852a112171"}},"metadata":{}},{"name":"stdout","text":"Epoch #8 loss: 18948071828624.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6991f7c941f443828cdf3242649d84b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2884183a9577441e91e6c5fd1377d609"}},"metadata":{}},{"name":"stdout","text":"Epoch #9 loss: 17799410685428.547\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92bc9bfc034b41e49fd7f191748972b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"705afadc60af4ab880708772e948bce7"}},"metadata":{}},{"name":"stdout","text":"Epoch #10 loss: 19731280424373.77\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c983a18737354ee8a08fcc51f11e8865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67f1b0f522ad4145b6b759de5ab9a40b"}},"metadata":{}},{"name":"stdout","text":"Epoch #11 loss: 17880433444451.55\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"547fc039916845c6b8ceb33db2f32da2"}},"metadata":{}}]},{"cell_type":"code","source":"img,target,_ = valid_dataset[5]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['boxes']))\nprint('real #boxes: ', len(target['boxes']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/makerere-passion-fruit-disease-detection-challenge/Test (12).csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(object):\n    def __init__(self, df, IMG_DIR, transforms):        \n        self.df = df\n        self.img_dir = IMG_DIR\n        self.transforms = transforms\n        self.image_ids = self.df['Image_ID'].tolist()\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):        \n        image_id = self.image_ids[idx]\n        image = cv2.imread(self.img_dir+image_id+\".jpg\",cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transform(IMG_SIZE=(512,512)):\n    return A.Compose([\n         A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n        A.Resize(*IMG_SIZE),\n        ToTensorV2(p=1.0)\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img_dir = \"../input/makerere-passion-fruit-disease-detection-challenge/Test_Images (1)/Test_Images/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = (512,512)\ntest_dataset = TestDataset(submission, test_img_dir ,get_test_transform())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\nfor j in range(submission.shape[0]):\n    \n    img,_ = test_dataset[j]\n    img = img.unsqueeze_(0)\n    # put the model in evaluation mode\n    model.eval()\n\n    with torch.no_grad():\n        prediction = model([img.to(device)][0])\n        aa = zip(prediction[0][\"boxes\"].tolist(), prediction[0][\"labels\"].tolist(), prediction[0][\"scores\"].tolist())\n       \n        for item in list(aa):\n            row_dict = {}\n            row_dict[\"Image_ID\"] = _\n            row_dict[\"boxes\"] = item[0]\n            row_dict[\"labels\"] = item[1]\n            row_dict[\"confidence\"] = item[2]\n            results.append(row_dict)\nsub_df = pd.DataFrame(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df[\"ymin\"] = sub_df[\"boxes\"].apply(lambda x: x[1])\nsub_df[\"xmin\"] = sub_df[\"boxes\"].apply(lambda x: x[0])\nsub_df[\"ymax\"] = sub_df[\"boxes\"].apply(lambda x: x[3])\nsub_df[\"xmax\"]=  sub_df[\"boxes\"].apply(lambda x: x[2])\nclasses_la = {0:\"Background\", 1:\"fruit_brownspot\", 2:\"fruit_healthy\", 3:\"fruit_woodiness\"}\nsub_df[\"labels\"] = sub_df[\"labels\"].apply(lambda x: classes_la[x])\nsub_df.drop([\"boxes\"], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.rename(columns={\"labels\":\"class\"}, inplace=True)\nsub_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\ndef create_submission(submission_file, submission_name):\n    submission_file.to_csv(submission_name+\".csv\",index=False)\n    return FileLink(submission_name+\".csv\")\ncreate_submission(sub_df, \"sub_df\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"Submission_20fgsdfg.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}