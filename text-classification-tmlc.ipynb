{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**import the necessary library**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-25T10:59:15.569004Z","iopub.execute_input":"2022-09-25T10:59:15.569433Z","iopub.status.idle":"2022-09-25T10:59:23.555940Z","shell.execute_reply.started":"2022-09-25T10:59:15.569343Z","shell.execute_reply":"2022-09-25T10:59:23.554843Z"}}},{"cell_type":"code","source":"import os\n\nimport random\nimport numpy as np\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import (\n    AdamW,\n    Trainer,\n    TrainingArguments,\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_cosine_schedule_with_warmup,\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport gc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**using hugging face roberta-large model , one of my favourite model that made state of the art result**","metadata":{}},{"cell_type":"code","source":"model_name = \"roberta-large\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, ignore_mismatched_sizes=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:26.068824Z","iopub.execute_input":"2022-09-25T10:59:26.070399Z","iopub.status.idle":"2022-09-25T10:59:28.177832Z","shell.execute_reply.started":"2022-09-25T10:59:26.070358Z","shell.execute_reply":"2022-09-25T10:59:28.176797Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c42a7636074c65a6e63b1e24a2eb33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3a783fd8def4715832bef95ec7d6646"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f65724a12584d39a0e74f1dc2a875be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62546f81c64d428c8755fcd95a2ed77c"}},"metadata":{}}]},{"cell_type":"markdown","source":"**preparing the necessary params with suitable values according to the dataset**","metadata":{}},{"cell_type":"code","source":"test_frac = 0.1\nlr = 2e-5\nepochs =  8\nbatch_size = 8\nmax_seq_len = 512","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:31.735777Z","iopub.execute_input":"2022-09-25T10:59:31.736684Z","iopub.status.idle":"2022-09-25T10:59:31.741594Z","shell.execute_reply.started":"2022-09-25T10:59:31.736647Z","shell.execute_reply":"2022-09-25T10:59:31.740478Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**prepare the env**","metadata":{}},{"cell_type":"code","source":"import os\n\ndef set_seed(seed=106052):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:32.422995Z","iopub.execute_input":"2022-09-25T10:59:32.423344Z","iopub.status.idle":"2022-09-25T10:59:32.432965Z","shell.execute_reply.started":"2022-09-25T10:59:32.423312Z","shell.execute_reply":"2022-09-25T10:59:32.432036Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**process the datatset : encode it and and made attention mask**","metadata":{}},{"cell_type":"code","source":"class CEFRDataset(Dataset):\n    \"\"\"Classification dataset, built on top of pytorch dataset object\n    \"\"\"\n    \n    def __init__(self, texts, labels):\n        \n        self.encoder = LabelEncoder()\n        print(self.encoder.__dict__)\n        self.texts = texts\n        self.labels = self.encoder.fit_transform(labels)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        label = self.labels[index]\n        encoded_text = tokenizer(\n            text,\n            padding=\"max_length\",\n            max_length=max_seq_len,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        encoded_text[\"input_ids\"] = encoded_text[\"input_ids\"].squeeze()\n        encoded_text[\"attention_mask\"] = encoded_text[\"attention_mask\"].squeeze()\n        label = torch.tensor(label)\n\n        return {\n            \"input_ids\": encoded_text[\"input_ids\"],\n            \"attention_mask\": encoded_text[\"attention_mask\"],\n            \"labels\": label,\n        }\n\n    def get_labels(self):\n        return self.labels","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:33.255808Z","iopub.execute_input":"2022-09-25T10:59:33.256803Z","iopub.status.idle":"2022-09-25T10:59:33.479816Z","shell.execute_reply.started":"2022-09-25T10:59:33.256757Z","shell.execute_reply":"2022-09-25T10:59:33.478842Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**fine tuning the model so it could give us better result**","metadata":{}},{"cell_type":"code","source":"def train(train_set, valid_set, epochs=10, warmup_size=0.1, lr=1e-3, batch_size=2):\n    model = get_model(model_name)\n    optim = AdamW(model.parameters(), lr=lr)\n    scheduler = get_scheduler(\n        optim, warmup_size, round(len(train_set) / batch_size * epochs)\n    )\n    training_args = get_training_args(epochs, batch_size)\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_set,\n        eval_dataset=valid_set,\n        optimizers=[optim, scheduler],\n        compute_metrics=compute_accuracy\n    )\n    trainer.train()\n    trainer.save_model()\n    return trainer","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:34.537004Z","iopub.execute_input":"2022-09-25T10:59:34.537365Z","iopub.status.idle":"2022-09-25T10:59:34.544479Z","shell.execute_reply.started":"2022-09-25T10:59:34.537332Z","shell.execute_reply":"2022-09-25T10:59:34.543120Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_model(pretrained_checkpoint):\n    model = AutoModelForSequenceClassification.from_pretrained(\n        pretrained_checkpoint, num_labels=4, ignore_mismatched_sizes=True\n    )\n    return model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:35.363738Z","iopub.execute_input":"2022-09-25T10:59:35.364403Z","iopub.status.idle":"2022-09-25T10:59:35.369333Z","shell.execute_reply.started":"2022-09-25T10:59:35.364369Z","shell.execute_reply":"2022-09-25T10:59:35.368352Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\ndef get_scheduler(optimizer, warmup_size, total_steps):\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=round(total_steps * warmup_size),\n        num_training_steps=total_steps,\n    )\n    return scheduler\n\n\ndef get_training_args(epochs, batch_size):\n    return TrainingArguments(\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        logging_steps=50,\n        fp16=False,\n        evaluation_strategy=\"epoch\",\n        eval_accumulation_steps=1,\n        report_to=None,\n#         save_total_limit=1,\n#         load_best_model_at_end=True,\n        save_strategy = 'epoch'\n    )\n\n\ndef compute_accuracy(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc}\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:36.006926Z","iopub.execute_input":"2022-09-25T10:59:36.007336Z","iopub.status.idle":"2022-09-25T10:59:36.018335Z","shell.execute_reply.started":"2022-09-25T10:59:36.007300Z","shell.execute_reply":"2022-09-25T10:59:36.017357Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def split_valid(df, frac=0.1):\n    \n    val = pd.DataFrame()\n    val[\"text\"] = \"\"\n    val[\"label\"] = -1\n    \n    for i in df.label.unique():\n        val = pd.concat([val, df[df.label == i].sample(frac=frac)])\n        \n    return df[~df.index.isin(val.index)].reset_index(drop=True) , val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:38.072666Z","iopub.execute_input":"2022-09-25T10:59:38.073053Z","iopub.status.idle":"2022-09-25T10:59:38.079520Z","shell.execute_reply.started":"2022-09-25T10:59:38.073019Z","shell.execute_reply":"2022-09-25T10:59:38.078517Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_set_df = pd.read_csv(\"../input/ecommerce-text-classification/ecommerceDataset.csv\")\ntrain_set_df.columns=[\"label\",\"text\"]","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:42.570372Z","iopub.execute_input":"2022-09-25T10:59:42.570745Z","iopub.status.idle":"2022-09-25T10:59:43.339962Z","shell.execute_reply.started":"2022-09-25T10:59:42.570710Z","shell.execute_reply":"2022-09-25T10:59:43.338996Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_set_df.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:44.298663Z","iopub.execute_input":"2022-09-25T10:59:44.299329Z","iopub.status.idle":"2022-09-25T10:59:44.319568Z","shell.execute_reply.started":"2022-09-25T10:59:44.299293Z","shell.execute_reply":"2022-09-25T10:59:44.318543Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Household                 19312\nBooks                     11820\nElectronics               10621\nClothing & Accessories     8671\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**split data**","metadata":{}},{"cell_type":"code","source":"train_set_df, valid_set_df = split_valid(train_set_df)","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:45.204695Z","iopub.execute_input":"2022-09-25T10:59:45.205372Z","iopub.status.idle":"2022-09-25T10:59:45.248659Z","shell.execute_reply.started":"2022-09-25T10:59:45.205336Z","shell.execute_reply":"2022-09-25T10:59:45.247761Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**encoding the label**","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntrain_set_df.label = le.fit_transform(train_set_df.label)\nvalid_set_df.label = le.transform(valid_set_df.label)","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:45.964308Z","iopub.execute_input":"2022-09-25T10:59:45.964800Z","iopub.status.idle":"2022-09-25T10:59:45.988027Z","shell.execute_reply.started":"2022-09-25T10:59:45.964757Z","shell.execute_reply":"2022-09-25T10:59:45.987039Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"valid_set_df","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:47.199349Z","iopub.execute_input":"2022-09-25T10:59:47.200117Z","iopub.status.idle":"2022-09-25T10:59:47.221653Z","shell.execute_reply.started":"2022-09-25T10:59:47.200072Z","shell.execute_reply":"2022-09-25T10:59:47.220565Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                                   text  label\n0     PEBBLE CRAFTS Handmade Wooden Kitchen Okhli Ma...      3\n1     BOIS ART Rosewood and Sheesham Wood Console Be...      3\n2     Ilu Dream Catcher Wall Hanging Handmade Beaded...      3\n3     Sanddune Boy's Cotton Hooded Half Sleeves Knee...      3\n4     DeckUp Bei 3-Door Shoe Rack with Wooden Legs (...      3\n...                                                 ...    ...\n5037  LAPSTER for SD TF Card Reader,USB to Lightning...      2\n5038  Logitech M90 USB Mouse (Dark Grey) High-defini...      2\n5039  FidgetGear 49mm 850nm Infrared Infra-Red IR Fi...      2\n5040  SanDisk Ultra A1 16GB Class 10 Ultra microSD U...      2\n5041  Mivi UC3B Micro USB Cable - 3.2 Feet (1 Meter)...      2\n\n[5042 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PEBBLE CRAFTS Handmade Wooden Kitchen Okhli Ma...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BOIS ART Rosewood and Sheesham Wood Console Be...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ilu Dream Catcher Wall Hanging Handmade Beaded...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sanddune Boy's Cotton Hooded Half Sleeves Knee...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DeckUp Bei 3-Door Shoe Rack with Wooden Legs (...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5037</th>\n      <td>LAPSTER for SD TF Card Reader,USB to Lightning...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5038</th>\n      <td>Logitech M90 USB Mouse (Dark Grey) High-defini...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5039</th>\n      <td>FidgetGear 49mm 850nm Infrared Infra-Red IR Fi...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5040</th>\n      <td>SanDisk Ultra A1 16GB Class 10 Ultra microSD U...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5041</th>\n      <td>Mivi UC3B Micro USB Cable - 3.2 Feet (1 Meter)...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5042 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm \n\ndef predict(model, text):\n    \n    preds = []\n    \n    for i in tqdm(range(len(text))):\n        tokenized = tokenizer(text[i:i+1], return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n        pred = model(**tokenized)\n        preds.append(pred.logits.argmax(-1).item())\n\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-09-25T10:59:47.954378Z","iopub.execute_input":"2022-09-25T10:59:47.955217Z","iopub.status.idle":"2022-09-25T10:59:47.965169Z","shell.execute_reply.started":"2022-09-25T10:59:47.955176Z","shell.execute_reply":"2022-09-25T10:59:47.963936Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**collect the cache**","metadata":{}},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-09-25T11:01:38.831332Z","iopub.execute_input":"2022-09-25T11:01:38.831695Z","iopub.status.idle":"2022-09-25T11:01:39.043846Z","shell.execute_reply.started":"2022-09-25T11:01:38.831663Z","shell.execute_reply":"2022-09-25T11:01:39.042916Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"7288"},"metadata":{}}]},{"cell_type":"markdown","source":"**train the model**","metadata":{}},{"cell_type":"code","source":"train_set = CEFRDataset(train_set_df[\"text\"], train_set_df[\"label\"])\nvalid_set = CEFRDataset(valid_set_df[\"text\"], valid_set_df[\"label\"])\n\n\ntrainer_second = train(train_set, valid_set, epochs=epochs, warmup_size=0.2, lr=lr, batch_size=1)\nmodel = trainer_second.model","metadata":{"execution":{"iopub.status.busy":"2022-09-25T11:01:40.839305Z","iopub.execute_input":"2022-09-25T11:01:40.839673Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\nModel config RobertaConfig {\n  \"_name_or_path\": \"roberta-large\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n","output_type":"stream"},{"name":"stdout","text":"{}\n{}\n","output_type":"stream"},{"name":"stderr","text":"loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n***** Running training *****\n  Num examples = 45382\n  Num Epochs = 8\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 1\n  Gradient Accumulation steps = 1\n  Total optimization steps = 363056\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3423' max='363056' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  3423/363056 15:38 < 27:25:01, 3.64 it/s, Epoch 0.08/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]}]}