{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport random\nimport numpy as np\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import (\n    AdamW,\n    Trainer,\n    TrainingArguments,\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_cosine_schedule_with_warmup,\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-01T13:08:53.920268Z","iopub.execute_input":"2022-09-01T13:08:53.921085Z","iopub.status.idle":"2022-09-01T13:08:53.926907Z","shell.execute_reply.started":"2022-09-01T13:08:53.921046Z","shell.execute_reply":"2022-09-01T13:08:53.925660Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# model_name = \"roberta-base\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name, ignore_mismatched_sizes=True)\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:53.932942Z","iopub.execute_input":"2022-09-01T13:08:53.933653Z","iopub.status.idle":"2022-09-01T13:08:53.938742Z","shell.execute_reply.started":"2022-09-01T13:08:53.933617Z","shell.execute_reply":"2022-09-01T13:08:53.937682Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"lr = 2e-5\nepochs =  6\nbatch_size = 5\nmax_seq_len = 75\n\ntest_frac = 0.1","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:53.940667Z","iopub.execute_input":"2022-09-01T13:08:53.941655Z","iopub.status.idle":"2022-09-01T13:08:53.947654Z","shell.execute_reply.started":"2022-09-01T13:08:53.941620Z","shell.execute_reply":"2022-09-01T13:08:53.946532Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef set_seed(seed=106052):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:53.949808Z","iopub.execute_input":"2022-09-01T13:08:53.950538Z","iopub.status.idle":"2022-09-01T13:08:53.959169Z","shell.execute_reply.started":"2022-09-01T13:08:53.950502Z","shell.execute_reply":"2022-09-01T13:08:53.958255Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"class CEFRDataset(Dataset):\n    \"\"\"Classification dataset, built on top of pytorch dataset object\n    \"\"\"\n    \n    def __init__(self, texts, labels):\n        \n        self.encoder = LabelEncoder()\n        print(self.encoder.__dict__)\n        self.texts = texts\n        self.labels = self.encoder.fit_transform(labels)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        label = self.labels[index]\n        encoded_text = tokenizer(\n            text,\n            padding=\"max_length\",\n            max_length=max_seq_len,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        encoded_text[\"input_ids\"] = encoded_text[\"input_ids\"].squeeze()\n        encoded_text[\"attention_mask\"] = encoded_text[\"attention_mask\"].squeeze()\n        label = torch.tensor(label)\n\n        return {\n            \"input_ids\": encoded_text[\"input_ids\"],\n            \"attention_mask\": encoded_text[\"attention_mask\"],\n            \"labels\": label,\n        }\n\n    def get_labels(self):\n        return self.labels","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:53.966226Z","iopub.execute_input":"2022-09-01T13:08:53.967014Z","iopub.status.idle":"2022-09-01T13:08:53.975626Z","shell.execute_reply.started":"2022-09-01T13:08:53.966975Z","shell.execute_reply":"2022-09-01T13:08:53.974650Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"train_set_df = pd.read_csv(\"../input/swahili-news-classification/Train (10).csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:53.977637Z","iopub.execute_input":"2022-09-01T13:08:53.978296Z","iopub.status.idle":"2022-09-01T13:08:54.115047Z","shell.execute_reply.started":"2022-09-01T13:08:53.978260Z","shell.execute_reply":"2022-09-01T13:08:54.114081Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"def train(train_set, valid_set, epochs=5, warmup_size=0.1, lr=1e-3, batch_size=8):\n    model = get_model(model_name)\n    optim = AdamW(model.parameters(), lr=lr)\n    scheduler = get_scheduler(\n        optim, warmup_size, round(len(train_set) / batch_size * epochs)\n    )\n    training_args = get_training_args(epochs, batch_size)\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_set,\n        eval_dataset=valid_set,\n        optimizers=[optim, scheduler],\n        compute_metrics=compute_accuracy\n    )\n    trainer.train()\n    trainer.save_model()\n    return trainer","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-09-01T13:08:54.116954Z","iopub.execute_input":"2022-09-01T13:08:54.117428Z","iopub.status.idle":"2022-09-01T13:08:54.124876Z","shell.execute_reply.started":"2022-09-01T13:08:54.117390Z","shell.execute_reply":"2022-09-01T13:08:54.123728Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def get_model(pretrained_checkpoint):\n    model = AutoModelForSequenceClassification.from_pretrained(\n        pretrained_checkpoint, num_labels=2, ignore_mismatched_sizes=True\n    )\n    return model.to(device)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-09-01T13:08:54.126337Z","iopub.execute_input":"2022-09-01T13:08:54.127042Z","iopub.status.idle":"2022-09-01T13:08:54.135219Z","shell.execute_reply.started":"2022-09-01T13:08:54.126999Z","shell.execute_reply":"2022-09-01T13:08:54.134135Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\ndef get_scheduler(optimizer, warmup_size, total_steps):\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=round(total_steps * warmup_size),\n        num_training_steps=total_steps,\n    )\n    return scheduler\n\n\ndef get_training_args(epochs, batch_size):\n    return TrainingArguments(\n        output_dir=\"./b\",\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        logging_steps=50,\n        fp16=False,\n        evaluation_strategy=\"epoch\",\n        eval_accumulation_steps=1,\n        report_to=None,\n#         save_total_limit=1,\n#         load_best_model_at_end=True,\n        save_strategy = 'epoch'\n    )\n\n\ndef compute_accuracy(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc}\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-09-01T13:08:54.138589Z","iopub.execute_input":"2022-09-01T13:08:54.139004Z","iopub.status.idle":"2022-09-01T13:08:54.147885Z","shell.execute_reply.started":"2022-09-01T13:08:54.138969Z","shell.execute_reply":"2022-09-01T13:08:54.146671Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"lr = 2e-5\nepochs =  8\nbatch_size = 8\nmax_seq_len = 512","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.149442Z","iopub.execute_input":"2022-09-01T13:08:54.149961Z","iopub.status.idle":"2022-09-01T13:08:54.160580Z","shell.execute_reply.started":"2022-09-01T13:08:54.149928Z","shell.execute_reply":"2022-09-01T13:08:54.159658Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/swahili-news-classification/Test (11).csv\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.161996Z","iopub.execute_input":"2022-09-01T13:08:54.162609Z","iopub.status.idle":"2022-09-01T13:08:54.207761Z","shell.execute_reply.started":"2022-09-01T13:08:54.162553Z","shell.execute_reply":"2022-09-01T13:08:54.206700Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"                                 swahili_id  \\\n0  001dd47ac202d9db6624a5ff734a5e7dddafeaf2   \n1  0043d97f7690e9bc02f0ed8bb2b260d1d44bad92   \n2  00579c2307b5c11003d21c40c3ecff5e922c3fd8   \n3  00868eeee349e286303706ef0ffd851f39708d37   \n4  00a5cb12d3058dcf2e42f277eee599992db32412   \n\n                                             content  \n0   MKUU wa Wilaya ya Bahi, Mkoani Dodoma, Mwanah...  \n1   MWISHONI mwa wiki hii, Timu ya Soka ya Taifa,...  \n2   THAMANI ya mauzo ya bidhaa za Afrika Masharik...  \n3   MENEJA Mawasiliano na Utetezi wa asasi ya AGP...  \n4   WAZIRI wa Kilimo, Japhet Hasunga amesema seri...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>swahili_id</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001dd47ac202d9db6624a5ff734a5e7dddafeaf2</td>\n      <td>MKUU wa Wilaya ya Bahi, Mkoani Dodoma, Mwanah...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0043d97f7690e9bc02f0ed8bb2b260d1d44bad92</td>\n      <td>MWISHONI mwa wiki hii, Timu ya Soka ya Taifa,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00579c2307b5c11003d21c40c3ecff5e922c3fd8</td>\n      <td>THAMANI ya mauzo ya bidhaa za Afrika Masharik...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00868eeee349e286303706ef0ffd851f39708d37</td>\n      <td>MENEJA Mawasiliano na Utetezi wa asasi ya AGP...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00a5cb12d3058dcf2e42f277eee599992db32412</td>\n      <td>WAZIRI wa Kilimo, Japhet Hasunga amesema seri...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_set_df.drop(\"id\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.209514Z","iopub.execute_input":"2022-09-01T13:08:54.209889Z","iopub.status.idle":"2022-09-01T13:08:54.216809Z","shell.execute_reply.started":"2022-09-01T13:08:54.209854Z","shell.execute_reply":"2022-09-01T13:08:54.215919Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"train_set_df.columns=[\"text\",\"label\"]\ntrain_set_df = train_set_df[[\"text\", \"label\"]]","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.218141Z","iopub.execute_input":"2022-09-01T13:08:54.219005Z","iopub.status.idle":"2022-09-01T13:08:54.225576Z","shell.execute_reply.started":"2022-09-01T13:08:54.218970Z","shell.execute_reply":"2022-09-01T13:08:54.224607Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\ntrain_set_df.text = train_set_df.text.apply(lambda x: x.replace(\"\\r\", \"\").replace(\"\\n\", \" \"))\n","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.226964Z","iopub.execute_input":"2022-09-01T13:08:54.228040Z","iopub.status.idle":"2022-09-01T13:08:54.246370Z","shell.execute_reply.started":"2022-09-01T13:08:54.228006Z","shell.execute_reply":"2022-09-01T13:08:54.245372Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"train_set_df.label = le.fit_transform(train_set_df.label)","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.251666Z","iopub.execute_input":"2022-09-01T13:08:54.253675Z","iopub.status.idle":"2022-09-01T13:08:54.260789Z","shell.execute_reply.started":"2022-09-01T13:08:54.253648Z","shell.execute_reply":"2022-09-01T13:08:54.259813Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \n\ndef predict(model, text):\n    \n    preds = []\n    \n    for i in tqdm(range(len(text))):\n        tokenized = tokenizer(text[i:i+1], return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n        pred = model(**tokenized)\n        preds.append(pred.logits.argmax(-1).item())\n\n    return preds","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-09-01T13:08:54.262246Z","iopub.execute_input":"2022-09-01T13:08:54.263139Z","iopub.status.idle":"2022-09-01T13:08:54.270369Z","shell.execute_reply.started":"2022-09-01T13:08:54.263075Z","shell.execute_reply":"2022-09-01T13:08:54.269374Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.271969Z","iopub.execute_input":"2022-09-01T13:08:54.272639Z","iopub.status.idle":"2022-09-01T13:08:54.546595Z","shell.execute_reply.started":"2022-09-01T13:08:54.272605Z","shell.execute_reply":"2022-09-01T13:08:54.545353Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"116"},"metadata":{}}]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.548072Z","iopub.execute_input":"2022-09-01T13:08:54.549071Z","iopub.status.idle":"2022-09-01T13:08:54.563649Z","shell.execute_reply.started":"2022-09-01T13:08:54.548963Z","shell.execute_reply":"2022-09-01T13:08:54.562333Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"train_set_df = reduce_mem_usage(train_set_df)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.566094Z","iopub.execute_input":"2022-09-01T13:08:54.566464Z","iopub.status.idle":"2022-09-01T13:08:54.579714Z","shell.execute_reply.started":"2022-09-01T13:08:54.566429Z","shell.execute_reply":"2022-09-01T13:08:54.578652Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Memory usage after optimization is: 0.04 MB\nDecreased by 43.7%\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install --upgrade transformers\n# !pip install simpletransformers","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:08:54.581251Z","iopub.execute_input":"2022-09-01T13:08:54.582233Z","iopub.status.idle":"2022-09-01T13:09:14.740703Z","shell.execute_reply.started":"2022-09-01T13:08:54.582196Z","shell.execute_reply":"2022-09-01T13:09:14.739501Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.21.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: simpletransformers in /opt/conda/lib/python3.7/site-packages (0.63.7)\nRequirement already satisfied: streamlit in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.12.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.21.6)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.12.1)\nRequirement already satisfied: tqdm>=4.47.0 in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (4.64.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2.28.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.3.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.7.3)\nRequirement already satisfied: seqeval in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.2.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.1.97)\nRequirement already satisfied: transformers>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (4.21.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2021.11.10)\nRequirement already satisfied: wandb>=0.10.32 in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.12.21)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2.1.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2.10.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.0.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.6.0->simpletransformers) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.6.0->simpletransformers) (0.8.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.6.0->simpletransformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.6.0->simpletransformers) (6.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.6.0->simpletransformers) (4.12.0)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (3.19.4)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (1.0.9)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (3.1.27)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (59.8.0)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (1.15.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (2.3)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (8.0.4)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (1.9.5)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.10.32->simpletransformers) (5.9.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (3.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets->simpletransformers) (3.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets->simpletransformers) (0.3.5.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->simpletransformers) (3.8.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->simpletransformers) (0.70.13)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets->simpletransformers) (2022.7.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->simpletransformers) (5.0.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets->simpletransformers) (0.18.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->simpletransformers) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->simpletransformers) (2022.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->simpletransformers) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->simpletransformers) (1.0.1)\nRequirement already satisfied: cachetools>=4.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (4.2.4)\nRequirement already satisfied: pympler>=0.9 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (1.0.1)\nRequirement already satisfied: validators>=0.2 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (0.20.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (0.10.2)\nRequirement already satisfied: pydeck>=0.1.dev5 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (0.8.0b1)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (12.1.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (9.1.1)\nRequirement already satisfied: watchdog in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (2.1.9)\nRequirement already satisfied: tornado>=5.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (6.1)\nRequirement already satisfied: blinker>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (1.4)\nRequirement already satisfied: typing-extensions>=3.10.0.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (4.3.0)\nRequirement already satisfied: tzlocal>=1.1 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (4.2)\nRequirement already satisfied: altair>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (4.2.0)\nRequirement already satisfied: semver in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (2.13.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->simpletransformers) (0.6.1)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard->simpletransformers) (0.37.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->simpletransformers) (3.3.7)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->simpletransformers) (2.2.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->simpletransformers) (1.8.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard->simpletransformers) (0.15.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->simpletransformers) (0.4.6)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->simpletransformers) (1.35.0)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->simpletransformers) (1.43.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.6.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.1.2)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.9)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.6.0->simpletransformers) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.9)\nRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from rich>=10.11.0->streamlit->simpletransformers) (0.9.1)\nRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.12.0)\nRequirement already satisfied: pytz-deprecation-shim in /opt/conda/lib/python3.7/site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.1.0.post0)\nRequirement already satisfied: backports.zoneinfo in /opt/conda/lib/python3.7/site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.2.1)\nRequirement already satisfied: decorator>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from validators>=0.2->streamlit->simpletransformers) (5.1.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->simpletransformers) (1.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->simpletransformers) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->simpletransformers) (1.7.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->simpletransformers) (0.13.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (3.0.5)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.1)\nRequirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (5.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.0)\nRequirement already satisfied: tzdata in /opt/conda/lib/python3.7/site-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit->simpletransformers) (2022.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport warnings\nwarnings.simplefilter('ignore')\nimport gc\nfrom scipy.special import softmax\nfrom simpletransformers.classification import ClassificationModel","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:09:14.744054Z","iopub.execute_input":"2022-09-01T13:09:14.744487Z","iopub.status.idle":"2022-09-01T13:09:14.751634Z","shell.execute_reply.started":"2022-09-01T13:09:14.744436Z","shell.execute_reply":"2022-09-01T13:09:14.750557Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"import sklearn\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:09:14.753191Z","iopub.execute_input":"2022-09-01T13:09:14.753716Z","iopub.status.idle":"2022-09-01T13:09:14.761445Z","shell.execute_reply.started":"2022-09-01T13:09:14.753681Z","shell.execute_reply":"2022-09-01T13:09:14.760440Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/swahili-news-classification/Test (11).csv\")\nid_=test.swahili_id\ntest1=test.drop(['swahili_id'],axis=1)\ntest1['label']=0","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:09:14.764404Z","iopub.execute_input":"2022-09-01T13:09:14.765505Z","iopub.status.idle":"2022-09-01T13:09:14.816127Z","shell.execute_reply.started":"2022-09-01T13:09:14.765448Z","shell.execute_reply":"2022-09-01T13:09:14.815156Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv(\"../input/swahili-news-classification/SampleSubmission (6).csv\")\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:09:14.817820Z","iopub.execute_input":"2022-09-01T13:09:14.818583Z","iopub.status.idle":"2022-09-01T13:09:14.836787Z","shell.execute_reply.started":"2022-09-01T13:09:14.818548Z","shell.execute_reply":"2022-09-01T13:09:14.835898Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"                                 swahili_id  kitaifa  michezo  biashara  \\\n0  001dd47ac202d9db6624a5ff734a5e7dddafeaf2        0        0         0   \n1  0043d97f7690e9bc02f0ed8bb2b260d1d44bad92        0        0         0   \n2  00579c2307b5c11003d21c40c3ecff5e922c3fd8        0        0         0   \n3  00868eeee349e286303706ef0ffd851f39708d37        0        0         0   \n4  00a5cb12d3058dcf2e42f277eee599992db32412        0        0         0   \n\n   kimataifa  burudani  \n0          0         0  \n1          0         0  \n2          0         0  \n3          0         0  \n4          0         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>swahili_id</th>\n      <th>kitaifa</th>\n      <th>michezo</th>\n      <th>biashara</th>\n      <th>kimataifa</th>\n      <th>burudani</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001dd47ac202d9db6624a5ff734a5e7dddafeaf2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0043d97f7690e9bc02f0ed8bb2b260d1d44bad92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00579c2307b5c11003d21c40c3ecff5e922c3fd8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00868eeee349e286303706ef0ffd851f39708d37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00a5cb12d3058dcf2e42f277eee599992db32412</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_set_df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:09:14.838045Z","iopub.execute_input":"2022-09-01T13:09:14.838740Z","iopub.status.idle":"2022-09-01T13:09:14.847397Z","shell.execute_reply.started":"2022-09-01T13:09:14.838703Z","shell.execute_reply":"2022-09-01T13:09:14.846387Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"3    2000\n4    1720\n0    1360\n2      54\n1      17\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"\nerr=[]\ny_pred_tot=[]\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfold=StratifiedKFold(n_splits=20, shuffle=True, random_state=2)\ni=1\nfor train_index, test_index in fold.split(train_set_df,train_set_df['label']):\n    train1_trn, train1_val = train_set_df.iloc[train_index], train_set_df.iloc[test_index]\n    model = ClassificationModel('roberta', 'roberta-large', use_cuda=True,num_labels=5, args={'train_batch_size':32,\n                                                                         'reprocess_input_data': True,\n                                                                         'overwrite_output_dir': True,\n                                                                         'fp16': False,\n                                                                         'do_lower_case': False,\n                                                                         'num_train_epochs': 2,\n                                                                         'max_seq_length': 64,\n                                                                         'regression': False,\n                                                                         'manual_seed': 2,\n                                                                         \"learning_rate\":3e-5,\n                                                                         'weight_decay':0,\n                                                                         \"save_eval_checkpoints\": False,\n                                                                         \"save_model_every_epoch\": False,\n                                                                         \"silent\": True})\n    model.train_model(train1_trn)\n    raw_outputs_val = model.eval_model(train1_val)[1]\n    raw_outputs_val = softmax(raw_outputs_val,axis=1)[:,:]\n#     print(f\"Log_Loss: {log_loss(train1_val['label'], raw_outputs_val)}\")\n#     err.append(log_loss(train1_val['label'], raw_outputs_val))\n    raw_outputs_test = model.eval_model(test1)[1]\n    raw_outputs_test = softmax(raw_outputs_test,axis=1)[:,:]\n    y_pred_tot.append(raw_outputs_test)\n# print(\"Mean LogLoss: \",np.mean(err))\nfinal=pd.DataFrame()\nfinal['swahili_id']=test['swahili_id']\nfinal['label']=y_pred_tot\nprint(final.shape)\nfinal.to_csv('20fold_rbl_2_3e5_32_64_0.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-01T13:20:19.224618Z","iopub.execute_input":"2022-09-01T13:20:19.225034Z","iopub.status.idle":"2022-09-01T14:44:52.268987Z","shell.execute_reply.started":"2022-09-01T13:20:19.224999Z","shell.execute_reply":"2022-09-01T14:44:52.266905Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3861967293.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'swahili_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'swahili_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mfinal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred_tot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'20fold_rbl_2_3e5_32_64_0.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \"\"\"\n\u001b[0;32m-> 3784\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m         if (\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4509\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         raise ValueError(\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;34m\"does not match length of index \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Length of values (20) does not match length of index (1288)"],"ename":"ValueError","evalue":"Length of values (20) does not match length of index (1288)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****","metadata":{}}]}