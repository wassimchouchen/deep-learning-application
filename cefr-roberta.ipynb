{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport random\nimport numpy as np\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import (\n    AdamW,\n    Trainer,\n    TrainingArguments,\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_cosine_schedule_with_warmup,\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-01T15:11:02.491430Z","iopub.execute_input":"2022-09-01T15:11:02.491817Z","iopub.status.idle":"2022-09-01T15:11:02.499338Z","shell.execute_reply.started":"2022-09-01T15:11:02.491783Z","shell.execute_reply":"2022-09-01T15:11:02.498348Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# model_name = \"roberta-base\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name, ignore_mismatched_sizes=True)\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:02.501787Z","iopub.execute_input":"2022-09-01T15:11:02.502609Z","iopub.status.idle":"2022-09-01T15:11:02.515998Z","shell.execute_reply.started":"2022-09-01T15:11:02.502571Z","shell.execute_reply":"2022-09-01T15:11:02.514815Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"lr = 2e-5\nepochs =  6\nbatch_size = 5\nmax_seq_len = 75\n\ntest_frac = 0.1","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:02.520046Z","iopub.execute_input":"2022-09-01T15:11:02.520454Z","iopub.status.idle":"2022-09-01T15:11:02.527671Z","shell.execute_reply.started":"2022-09-01T15:11:02.520425Z","shell.execute_reply":"2022-09-01T15:11:02.526669Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef set_seed(seed=106052):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:02.565795Z","iopub.execute_input":"2022-09-01T15:11:02.566369Z","iopub.status.idle":"2022-09-01T15:11:02.572856Z","shell.execute_reply.started":"2022-09-01T15:11:02.566315Z","shell.execute_reply":"2022-09-01T15:11:02.571844Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"class CEFRDataset(Dataset):\n    \"\"\"Classification dataset, built on top of pytorch dataset object\n    \"\"\"\n    \n    def __init__(self, texts, labels):\n        \n        self.encoder = LabelEncoder()\n        print(self.encoder.__dict__)\n        self.texts = texts\n        self.labels = self.encoder.fit_transform(labels)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        label = self.labels[index]\n        encoded_text = tokenizer(\n            text,\n            padding=\"max_length\",\n            max_length=max_seq_len,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        encoded_text[\"input_ids\"] = encoded_text[\"input_ids\"].squeeze()\n        encoded_text[\"attention_mask\"] = encoded_text[\"attention_mask\"].squeeze()\n        label = torch.tensor(label)\n\n        return {\n            \"input_ids\": encoded_text[\"input_ids\"],\n            \"attention_mask\": encoded_text[\"attention_mask\"],\n            \"labels\": label,\n        }\n\n    def get_labels(self):\n        return self.labels","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:02.584425Z","iopub.execute_input":"2022-09-01T15:11:02.585232Z","iopub.status.idle":"2022-09-01T15:11:02.593825Z","shell.execute_reply.started":"2022-09-01T15:11:02.585205Z","shell.execute_reply":"2022-09-01T15:11:02.592726Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"train_set_df = pd.read_csv(\"../input/swahili-news-classification/Train (10).csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:02.596114Z","iopub.execute_input":"2022-09-01T15:11:02.596611Z","iopub.status.idle":"2022-09-01T15:11:02.856399Z","shell.execute_reply.started":"2022-09-01T15:11:02.596570Z","shell.execute_reply":"2022-09-01T15:11:02.855181Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"def train(train_set, valid_set, epochs=5, warmup_size=0.1, lr=1e-3, batch_size=8):\n    model = get_model(model_name)\n    optim = AdamW(model.parameters(), lr=lr)\n    scheduler = get_scheduler(\n        optim, warmup_size, round(len(train_set) / batch_size * epochs)\n    )\n    training_args = get_training_args(epochs, batch_size)\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_set,\n        eval_dataset=valid_set,\n        optimizers=[optim, scheduler],\n        compute_metrics=compute_accuracy\n    )\n    trainer.train()\n    trainer.save_model()\n    return trainer","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-09-01T15:11:02.858630Z","iopub.execute_input":"2022-09-01T15:11:02.859013Z","iopub.status.idle":"2022-09-01T15:11:02.867576Z","shell.execute_reply.started":"2022-09-01T15:11:02.858965Z","shell.execute_reply":"2022-09-01T15:11:02.864917Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"def get_model(pretrained_checkpoint):\n    model = AutoModelForSequenceClassification.from_pretrained(\n        pretrained_checkpoint, num_labels=2, ignore_mismatched_sizes=True\n    )\n    return model.to(device)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-09-01T15:11:02.868886Z","iopub.execute_input":"2022-09-01T15:11:02.869156Z","iopub.status.idle":"2022-09-01T15:11:02.877650Z","shell.execute_reply.started":"2022-09-01T15:11:02.869132Z","shell.execute_reply":"2022-09-01T15:11:02.876627Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\ndef get_scheduler(optimizer, warmup_size, total_steps):\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=round(total_steps * warmup_size),\n        num_training_steps=total_steps,\n    )\n    return scheduler\n\n\ndef get_training_args(epochs, batch_size):\n    return TrainingArguments(\n        output_dir=\"./b\",\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        logging_steps=50,\n        fp16=False,\n        evaluation_strategy=\"epoch\",\n        eval_accumulation_steps=1,\n        report_to=None,\n#         save_total_limit=1,\n#         load_best_model_at_end=True,\n        save_strategy = 'epoch'\n    )\n\n\ndef compute_accuracy(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc}\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-09-01T15:11:02.880964Z","iopub.execute_input":"2022-09-01T15:11:02.881491Z","iopub.status.idle":"2022-09-01T15:11:02.889267Z","shell.execute_reply.started":"2022-09-01T15:11:02.881459Z","shell.execute_reply":"2022-09-01T15:11:02.888315Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"lr = 2e-5\nepochs =  8\nbatch_size = 8\nmax_seq_len = 512","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:02.890571Z","iopub.execute_input":"2022-09-01T15:11:02.891162Z","iopub.status.idle":"2022-09-01T15:11:02.901532Z","shell.execute_reply.started":"2022-09-01T15:11:02.891127Z","shell.execute_reply":"2022-09-01T15:11:02.900479Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/swahili-news-classification/Test (11).csv\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:02.903176Z","iopub.execute_input":"2022-09-01T15:11:02.903633Z","iopub.status.idle":"2022-09-01T15:11:02.978170Z","shell.execute_reply.started":"2022-09-01T15:11:02.903600Z","shell.execute_reply":"2022-09-01T15:11:02.977126Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"                                 swahili_id  \\\n0  001dd47ac202d9db6624a5ff734a5e7dddafeaf2   \n1  0043d97f7690e9bc02f0ed8bb2b260d1d44bad92   \n2  00579c2307b5c11003d21c40c3ecff5e922c3fd8   \n3  00868eeee349e286303706ef0ffd851f39708d37   \n4  00a5cb12d3058dcf2e42f277eee599992db32412   \n\n                                             content  \n0   MKUU wa Wilaya ya Bahi, Mkoani Dodoma, Mwanah...  \n1   MWISHONI mwa wiki hii, Timu ya Soka ya Taifa,...  \n2   THAMANI ya mauzo ya bidhaa za Afrika Masharik...  \n3   MENEJA Mawasiliano na Utetezi wa asasi ya AGP...  \n4   WAZIRI wa Kilimo, Japhet Hasunga amesema seri...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>swahili_id</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001dd47ac202d9db6624a5ff734a5e7dddafeaf2</td>\n      <td>MKUU wa Wilaya ya Bahi, Mkoani Dodoma, Mwanah...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0043d97f7690e9bc02f0ed8bb2b260d1d44bad92</td>\n      <td>MWISHONI mwa wiki hii, Timu ya Soka ya Taifa,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00579c2307b5c11003d21c40c3ecff5e922c3fd8</td>\n      <td>THAMANI ya mauzo ya bidhaa za Afrika Masharik...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00868eeee349e286303706ef0ffd851f39708d37</td>\n      <td>MENEJA Mawasiliano na Utetezi wa asasi ya AGP...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00a5cb12d3058dcf2e42f277eee599992db32412</td>\n      <td>WAZIRI wa Kilimo, Japhet Hasunga amesema seri...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_set_df.drop(\"id\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:02.979506Z","iopub.execute_input":"2022-09-01T15:11:02.979853Z","iopub.status.idle":"2022-09-01T15:11:02.986539Z","shell.execute_reply.started":"2022-09-01T15:11:02.979819Z","shell.execute_reply":"2022-09-01T15:11:02.985180Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"train_set_df.columns=[\"text\",\"label\"]\ntrain_set_df = train_set_df[[\"text\", \"label\"]]","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:02.988674Z","iopub.execute_input":"2022-09-01T15:11:02.989126Z","iopub.status.idle":"2022-09-01T15:11:02.997588Z","shell.execute_reply.started":"2022-09-01T15:11:02.989090Z","shell.execute_reply":"2022-09-01T15:11:02.996553Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\ntrain_set_df.text = train_set_df.text.apply(lambda x: x.replace(\"\\r\", \"\").replace(\"\\n\", \" \"))\n","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.000263Z","iopub.execute_input":"2022-09-01T15:11:03.000688Z","iopub.status.idle":"2022-09-01T15:11:03.018880Z","shell.execute_reply.started":"2022-09-01T15:11:03.000654Z","shell.execute_reply":"2022-09-01T15:11:03.018024Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"train_set_df.label = le.fit_transform(train_set_df.label)","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.022545Z","iopub.execute_input":"2022-09-01T15:11:03.022803Z","iopub.status.idle":"2022-09-01T15:11:03.029283Z","shell.execute_reply.started":"2022-09-01T15:11:03.022780Z","shell.execute_reply":"2022-09-01T15:11:03.028293Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm \n\ndef predict(model, text):\n    \n    preds = []\n    \n    for i in tqdm(range(len(text))):\n        tokenized = tokenizer(text[i:i+1], return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n        pred = model(**tokenized)\n        preds.append(pred.logits.argmax(-1).item())\n\n    return preds","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-09-01T15:11:03.031000Z","iopub.execute_input":"2022-09-01T15:11:03.031689Z","iopub.status.idle":"2022-09-01T15:11:03.038953Z","shell.execute_reply.started":"2022-09-01T15:11:03.031653Z","shell.execute_reply":"2022-09-01T15:11:03.037902Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.040415Z","iopub.execute_input":"2022-09-01T15:11:03.040865Z","iopub.status.idle":"2022-09-01T15:11:03.561445Z","shell.execute_reply.started":"2022-09-01T15:11:03.040830Z","shell.execute_reply":"2022-09-01T15:11:03.560445Z"},"trusted":true},"execution_count":97,"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"243"},"metadata":{}}]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.563174Z","iopub.execute_input":"2022-09-01T15:11:03.563606Z","iopub.status.idle":"2022-09-01T15:11:03.578304Z","shell.execute_reply.started":"2022-09-01T15:11:03.563547Z","shell.execute_reply":"2022-09-01T15:11:03.577353Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"train_set_df = reduce_mem_usage(train_set_df)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.580234Z","iopub.execute_input":"2022-09-01T15:11:03.580739Z","iopub.status.idle":"2022-09-01T15:11:03.593035Z","shell.execute_reply.started":"2022-09-01T15:11:03.580695Z","shell.execute_reply":"2022-09-01T15:11:03.591985Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Memory usage after optimization is: 0.04 MB\nDecreased by 43.7%\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nenc=OneHotEncoder()\n\ntrain_set_df= pd.get_dummies(train_set_df, columns=[\"label\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.594232Z","iopub.execute_input":"2022-09-01T15:11:03.595379Z","iopub.status.idle":"2022-09-01T15:11:03.608819Z","shell.execute_reply.started":"2022-09-01T15:11:03.595343Z","shell.execute_reply":"2022-09-01T15:11:03.607826Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# !pip install --upgrade transformers\n# !pip install simpletransformers","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.610025Z","iopub.execute_input":"2022-09-01T15:11:03.610804Z","iopub.status.idle":"2022-09-01T15:11:03.617050Z","shell.execute_reply.started":"2022-09-01T15:11:03.610769Z","shell.execute_reply":"2022-09-01T15:11:03.616016Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport warnings\nwarnings.simplefilter('ignore')\nimport gc\nfrom scipy.special import softmax\nfrom simpletransformers.classification import ClassificationModel","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.619624Z","iopub.execute_input":"2022-09-01T15:11:03.620027Z","iopub.status.idle":"2022-09-01T15:11:03.627538Z","shell.execute_reply.started":"2022-09-01T15:11:03.619994Z","shell.execute_reply":"2022-09-01T15:11:03.626582Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"import sklearn\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.629311Z","iopub.execute_input":"2022-09-01T15:11:03.629802Z","iopub.status.idle":"2022-09-01T15:11:03.639538Z","shell.execute_reply.started":"2022-09-01T15:11:03.629769Z","shell.execute_reply":"2022-09-01T15:11:03.638367Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/swahili-news-classification/Test (11).csv\")\nid_=test.swahili_id\ntest1=test.drop(['swahili_id'],axis=1)\ntest1['label']=0","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.641371Z","iopub.execute_input":"2022-09-01T15:11:03.641722Z","iopub.status.idle":"2022-09-01T15:11:03.683481Z","shell.execute_reply.started":"2022-09-01T15:11:03.641689Z","shell.execute_reply":"2022-09-01T15:11:03.682535Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv(\"../input/swahili-news-classification/SampleSubmission (6).csv\")\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.684974Z","iopub.execute_input":"2022-09-01T15:11:03.685357Z","iopub.status.idle":"2022-09-01T15:11:03.706641Z","shell.execute_reply.started":"2022-09-01T15:11:03.685305Z","shell.execute_reply":"2022-09-01T15:11:03.705782Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"                                 swahili_id  kitaifa  michezo  biashara  \\\n0  001dd47ac202d9db6624a5ff734a5e7dddafeaf2        0        0         0   \n1  0043d97f7690e9bc02f0ed8bb2b260d1d44bad92        0        0         0   \n2  00579c2307b5c11003d21c40c3ecff5e922c3fd8        0        0         0   \n3  00868eeee349e286303706ef0ffd851f39708d37        0        0         0   \n4  00a5cb12d3058dcf2e42f277eee599992db32412        0        0         0   \n\n   kimataifa  burudani  \n0          0         0  \n1          0         0  \n2          0         0  \n3          0         0  \n4          0         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>swahili_id</th>\n      <th>kitaifa</th>\n      <th>michezo</th>\n      <th>biashara</th>\n      <th>kimataifa</th>\n      <th>burudani</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001dd47ac202d9db6624a5ff734a5e7dddafeaf2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0043d97f7690e9bc02f0ed8bb2b260d1d44bad92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00579c2307b5c11003d21c40c3ecff5e922c3fd8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00868eeee349e286303706ef0ffd851f39708d37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00a5cb12d3058dcf2e42f277eee599992db32412</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_set_df.columns=[\"text\",\"Biashara\",\"Burudani\",\"Kimataifa\",\"Kitaifa\",\"michezo\"]","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.708184Z","iopub.execute_input":"2022-09-01T15:11:03.708814Z","iopub.status.idle":"2022-09-01T15:11:03.713764Z","shell.execute_reply.started":"2022-09-01T15:11:03.708774Z","shell.execute_reply":"2022-09-01T15:11:03.712474Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"err=[]\ny_pred_tot=[]\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfold=StratifiedKFold(n_splits=10, shuffle=True, random_state=2)\ni=1\nfor train_index, test_index in fold.split(train_set_df,train_set_df[\"Biashara\"]):\n    train1_trn, train1_val = train_set_df.iloc[train_index], train_set_df.iloc[test_index]\n    model = ClassificationModel('roberta', 'roberta-large', use_cuda=True,num_labels=5, args={'train_batch_size':32,\n                                                                         'reprocess_input_data': True,\n                                                                         'overwrite_output_dir': True,\n                                                                         'fp16': False,\n                                                                         'do_lower_case': False,\n                                                                         'num_train_epochs': 2,\n                                                                         'max_seq_length': 64,\n                                                                         'regression': False,\n                                                                         'manual_seed': 2,\n                                                                         \"learning_rate\":3e-5,\n                                                                         'weight_decay':0,\n                                                                         \"save_eval_checkpoints\": False,\n                                                                         \"save_model_every_epoch\": False,\n                                                                         \"silent\": True})\n    model.train_model(train1_trn)\n\nraw_outputs_test = model.eval_model(test1)[1]\nfinal=pd.DataFrame()\nfinal['swahili_id']=test['swahili_id']\nfinal[[\"Biashara\",\"Burudani\",\"Kimataifa\",\"Kitaifa\",\"michezo\"]]=raw_outputs_test[1]\nprint(final.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-01T15:11:03.715469Z","iopub.execute_input":"2022-09-01T15:11:03.715898Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"final.to_csv('3.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****","metadata":{}}]}